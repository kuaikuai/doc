* http://lxr.free-electrons.com/ident
* ip_options_build
#+BEGIN_SRC c 
void ip_options_build(struct sk_buff * skb, struct ip_options * opt,
			    u32 daddr, struct rtable *rt, int is_frag) 
{
	unsigned char * iph = skb->nh.raw;

	memcpy(&(IPCB(skb)->opt), opt, sizeof(struct ip_options));
	memcpy(iph+sizeof(struct iphdr), opt->__data, opt->optlen);
	opt = &(IPCB(skb)->opt);
	opt->is_data = 0;

	if (opt->srr)
		memcpy(iph+opt->srr+iph[opt->srr+1]-4, &daddr, 4);

	if (!is_frag) {
		if (opt->rr_needaddr)
			ip_rt_get_source(iph+opt->rr+iph[opt->rr+2]-5, rt);
		if (opt->ts_needaddr)
			ip_rt_get_source(iph+opt->ts+iph[opt->ts+2]-9, rt);
		if (opt->ts_needtime) {
			struct timeval tv;
			__u32 midtime;
			do_gettimeofday(&tv);
			midtime = htonl((tv.tv_sec % 86400) * 1000 + tv.tv_usec / 1000);
			memcpy(iph+opt->ts+iph[opt->ts+2]-5, &midtime, 4);
		}
		return;
	}
	if (opt->rr) {
		memset(iph+opt->rr, IPOPT_NOP, iph[opt->rr+1]);
		opt->rr = 0;
		opt->rr_needaddr = 0;
	}
	if (opt->ts) {
		memset(iph+opt->ts, IPOPT_NOP, iph[opt->ts+1]);
		opt->ts = 0;
		opt->ts_needaddr = opt->ts_needtime = 0;
	}
}

#+END_SRC
memcpy(iph+opt->srr+iph[opt->srr+1]-4, &daddr, 4);
opt->srr是srr在ip头中的偏移值
iph+opt->srr是指向了srr，
iph[opt->srr+1]获取了srr的长度
所以
iph+opt->srr+iph[opt->srr+1]-4 是指向srr的选项列表的最后一个项

memcpy(iph+opt->ts+iph[opt->ts+2]-5, &midtime, 4);
iph[opt->ts+2]获取ts指针的值，该指针实际上是1开始的偏移值，所以需要 -1 -4 = -5
* netdev_max_backlog作用

Receive packet steering简称rps，是google贡献给linux kernel的一个patch，主要的功能是解决多核情况下，网络协议栈的软中断的负载均衡。这里的负载均衡也就是指能够将软中断均衡的放在不同的cpu核心上运行。

简介在这里：
http://lwn.net/Articles/362339/

linux现在网卡的驱动支持两种模式，一种是NAPI，一种是非NAPI模式，这两种模式的区别，我前面的blog都有介绍，这里就再次简要的介绍下。

在NAPI中，中断收到数据包后调用__napi_schedule调度软中断，然后软中断处理函数中会调用注册的poll回掉函数中调用netif_receive_skb将数据包发送到3层，没有进行任何的软中断负载均衡。

在非NAPI中，中断收到数据包后调用netif_rx，这个函数会将数据包保存到input_pkt_queue，然后调度软中断，这里为了兼容NAPI的驱动，他的poll方法默认是process_backlog，最终这个函数会从input_pkt_queue中取得数据包然后发送到3层。

通过比较我们可以看到，不管是NAPI还是非NAPI的话都无法做到软中断的负载均衡，因为软中断此时都是运行在在硬件中断相应的cpu上。也就是说如果始终是cpu0相应网卡的硬件中断，那么始终都是cpu0在处理软中断，而此时cpu1就被浪费了，因为无法并行的执行多个软中断。

google的这个patch的基本原理是这样的,根据数据包的源地址，目的地址以及目的和源端口(这里它是将两个端口组合成一个4字节的无符数进行计算的，后面会看到)计算出一个hash值，然后根据这个hash值来选择软中断运行的cpu，从上层来看，也就是说将每个连接和cpu绑定，并通过这个hash值，来均衡软中断在多个cpu上。

这个介绍比较简单，我们来看代码是如何实现的。

它这里主要是hook了两个内核的函数，一个是netif_rx主要是针对非NAPI的驱动，一个是netif_receive_skb这个主要是针对NAPI的驱动。

在看netif_rx和netif_receive_skb之前，我们先来看这个patch中两个重要的函数get_rps_cpu和enqueue_to_backlog，我们一个个看。

先来看相关的两个数据结构，首先是netdev_rx_queue，它表示对应的接收队列，因为有的网卡可能硬件上就支持多队列的模式，此时对应就会有多个rx队列，这个结构是挂载在net_device中的，也就是每个网络设备最终都会有一个或者多个rx队列。这个结构在sys文件系统中的表示类似这样的/sys/class/net/<device>/queues/rx-<n> 几个队列就是rx-n.

#+begin_src c
    struct netdev_rx_queue {  
    //保存了当前队列的rps map  
        struct rps_map *rps_map;  
    //对应的kobject  
        struct kobject kobj;  
    //指向第一个rx队列  
        struct netdev_rx_queue *first;  
    //引用计数  
        atomic_t count;  
    } ____cacheline_aligned_in_smp;  
#+end_src


然后就是rps_map，其实这个也就是保存了能够执行数据包的cpu。
#+begin_src c
    struct rps_map {  
    //cpu的个数，也就是cpus数组的个数  
        unsigned int len;  
    //RCU锁  
        struct rcu_head rcu;  
    //保存了cpu的id.  
        u16 cpus[0];  
    };  
#+end_src


看完上面的结构，我们来看函数的实现。
get_rps_cpu主要是通过传递进来的skb然后来选择这个skb所应该被处理的cpu。它的逻辑很简单，就是通过skb计算hash，然后通过hash从对应的队列的rps_mapping中取得对应的cpu id。

这里有个要注意的就是这个hash值是可以交给硬件网卡去计算的，作者自己说是最好交由硬件去计算这个hash值，因为如果是软件计算的话会导致CPU 缓存不命中，带来一定的性能开销。

还有就是rps_mapping这个值是可以通过sys 文件系统设置的，位置在这里：
/sys/class/net/<device>/queues/rx-<n>/rps_cpus 。

#+begin_src c
    static int get_rps_cpu(struct net_device *dev, struct sk_buff *skb)  
    {  
        struct ipv6hdr *ip6;  
        struct iphdr *ip;  
        struct netdev_rx_queue *rxqueue;  
        struct rps_map *map;  
        int cpu = -1;  
        u8 ip_proto;  
        u32 addr1, addr2, ports, ihl;  
    //rcu锁  
        rcu_read_lock();  
    //取得设备对应的rx 队列  
        if (skb_rx_queue_recorded(skb)) {  
        ..........................................  
            rxqueue = dev->_rx + index;  
        } else  
            rxqueue = dev->_rx;  
      
        if (!rxqueue->rps_map)  
            goto done;  
    //如果硬件已经计算，则跳过计算过程  
        if (skb->rxhash)  
            goto got_hash; /* Skip hash computation on packet header */  
      
        switch (skb->protocol) {  
        case __constant_htons(ETH_P_IP):  
            if (!pskb_may_pull(skb, sizeof(*ip)))  
                goto done;  
    //得到计算hash的几个值  
            ip = (struct iphdr *) skb->data;  
            ip_proto = ip->protocol;  
    //两个地址  
            addr1 = ip->saddr;  
            addr2 = ip->daddr;  
    //得到ip头  
            ihl = ip->ihl;  
            break;  
        case __constant_htons(ETH_P_IPV6):  
    ..........................................  
            break;  
        default:  
            goto done;  
        }  
        ports = 0;  
        switch (ip_proto) {  
        case IPPROTO_TCP:  
        case IPPROTO_UDP:  
        case IPPROTO_DCCP:  
        case IPPROTO_ESP:  
        case IPPROTO_AH:  
        case IPPROTO_SCTP:  
        case IPPROTO_UDPLITE:  
            if (pskb_may_pull(skb, (ihl * 4) + 4))  
    //我们知道tcp头的前4个字节就是源和目的端口，因此这里跳过ip头得到tcp头的前4个字节  
                ports = *((u32 *) (skb->data + (ihl * 4)));  
            break;  
      
        default:  
            break;  
        }  
    //计算hash  
        skb->rxhash = jhash_3words(addr1, addr2, ports, hashrnd);  
        if (!skb->rxhash)  
            skb->rxhash = 1;  
      
    got_hash:  
    //通过rcu得到对应rps map  
        map = rcu_dereference(rxqueue->rps_map);  
        if (map) {  
    //取得对应的cpu  
            u16 tcpu = map->cpus[((u64) skb->rxhash * map->len) >> 32];  
    //如果cpu是online的，则返回计算出的这个cpu，否则跳出循环。  
            if (cpu_online(tcpu)) {  
                cpu = tcpu;  
                goto done;  
            }  
        }  
      
    done:  
        rcu_read_unlock();  
    //如果上面失败，则返回-1.  
        return cpu;  
    }  
#+end_src


然后是enqueue_to_backlog这个方法，首先我们知道在每个cpu都有一个softnet结构，而他有一个input_pkt_queue的队列，以前这个主要是用于非NAPi的驱动的，而这个patch则将这个队列也用与NAPI的处理中了。也就是每个cpu现在都会有一个input_pkt_queue队列，用于保存需要处理的数据包队列。这个队列作用现在是，如果发现不属于当前cpu处理的数据包，则我们可以直接将数据包挂载到他所属的cpu的input_pkt_queue中。

enqueue_to_backlog接受一个skb和cpu为参数，通过cpu来判断skb如何处理。要么加入所属的input_pkt_queue中，要么schecule 软中断。

还有个要注意就是我们知道NAPI为了兼容非NAPI模式，有个backlog的napi_struct结构，也就是非NAPI驱动会schedule backlog这个napi结构，而在enqueue_to_backlog中则是利用了这个结构，也就是它会schedule backlog，因为它会将数据放到input_pkt_queue中，而backlog的pool方法process_backlog就是从input_pkt_queue中取得数据然后交给上层处理。

这里还有一个会用到结构就是 rps_remote_softirq_cpus，它主要是保存了当前cpu上需要去另外的cpu schedule 软中断的cpu 掩码。因为我们可能将要处理的数据包放到了另外的cpu的input queue上，因此我们需要schedule 另外的cpu上的napi(也就是软中断),所以我们需要保存对应的cpu掩码，以便于后面遍历，然后schedule。

而这里为什么mask有两个元素，注释写的很清楚：
#+begin_src c
    /* 
     * This structure holds the per-CPU mask of CPUs for which IPIs are scheduled 
     * to be sent to kick remote softirq processing.  There are two masks since 
     * the sending of IPIs must be done with interrupts enabled.  The select field 
     * indicates the current mask that enqueue_backlog uses to schedule IPIs. 
     * select is flipped before net_rps_action is called while still under lock, 
     * net_rps_action then uses the non-selected mask to send the IPIs and clears 
     * it without conflicting with enqueue_backlog operation. 
     */  
    struct rps_remote_softirq_cpus {  
    //对应的cpu掩码  
        cpumask_t mask[2];  
    //表示应该使用的数组索引  
        int select;  
    };  
#+end_src

#+begin_src c
    static int enqueue_to_backlog(struct sk_buff *skb, int cpu)  
    {  
        struct softnet_data *queue;  
        unsigned long flags;  
    //取出传递进来的cpu的softnet-data结构  
        queue = &per_cpu(softnet_data, cpu);  
      
        local_irq_save(flags);  
        __get_cpu_var(netdev_rx_stat).total++;  
    //自旋锁  
        spin_lock(&queue->input_pkt_queue.lock);  
    //如果保存的队列还没到上限  
        if (queue->input_pkt_queue.qlen <= netdev_max_backlog) {  
    //如果当前队列的输入队列长度不为空  
            if (queue->input_pkt_queue.qlen) {  
    enqueue:  
    //将数据包加入到input_pkt_queue中,这里会有一个小问题，我们后面再说。  
                __skb_queue_tail(&queue->input_pkt_queue, skb);  
                spin_unlock_irqrestore(&queue->input_pkt_queue.lock,  
                    flags);  
                return NET_RX_SUCCESS;  
            }  
      
            /* Schedule NAPI for backlog device */  
    //如果可以调度软中断  
            if (napi_schedule_prep(&queue->backlog)) {  
    //首先判断数据包该不该当前的cpu处理  
                if (cpu != smp_processor_id()) {  
    //如果不该，  
                    struct rps_remote_softirq_cpus *rcpus =  
                        &__get_cpu_var(rps_remote_softirq_cpus);  
      
                    cpu_set(cpu, rcpus->mask[rcpus->select]);  
                    __raise_softirq_irqoff(NET_RX_SOFTIRQ);  
                } else  
    //如果就是应该当前cpu处理，则直接schedule 软中断，这里可以看到传递进去的是backlog  
                    __napi_schedule(&queue->backlog);  
            }  
            goto enqueue;  
        }  
      
        spin_unlock(&queue->input_pkt_queue.lock);  
      
        __get_cpu_var(netdev_rx_stat).dropped++;  
        local_irq_restore(flags);  
      
        kfree_skb(skb);  
        return NET_RX_DROP;  
    }  
#+end_src

这里会有一个小问题，那就是假设此时一个属于cpu0的包进入处理，此时我们运行在cpu1,此时将数据包加入到input队列，然后cpu0上面刚好又来了一个cpu0需要处理的数据包，此时由于qlen不为0则又将数据包加入到input队列中，我们会发现cpu0上的napi没机会进行调度了。

google的patch对这个是这样处理的，在软中断处理函数中当数据包处理完毕，会调用net_rps_action来调度前面保存到其他cpu上的input队列。

下面就是代码片断（net_rx_action）

#+begin_src c
    //得到对应的rcpus.  
    rcpus = &__get_cpu_var(rps_remote_softirq_cpus);  
        select = rcpus->select;  
    //翻转select，防止和enqueue_backlog冲突  
        rcpus->select ^= 1;  
      
    //打开中断，此时下面的调度才会起作用.  
        local_irq_enable();  
    //这个函数里面调度对应的远程cpu的napi.  
        net_rps_action(&rcpus->mask[select]);  
#+end_src


然后就是net_rps_action，这个函数很简单，就是遍历所需要处理的cpu，然后调度napi
#+begin_src c
    static void net_rps_action(cpumask_t *mask)  
    {  
        int cpu;  
      
        /* Send pending IPI's to kick RPS processing on remote cpus. */  
    //遍历  
        for_each_cpu_mask_nr(cpu, *mask) {  
            struct softnet_data *queue = &per_cpu(softnet_data, cpu);  
            if (cpu_online(cpu))  
    //到对应的cpu调用csd方法。  
                __smp_call_function_single(cpu, &queue->csd, 0);  
        }  
    //清理mask  
        cpus_clear(*mask);  
    }  
#+end_src

上面我们看到会调用csd方法，而上面的csd回掉就是被初始化为trigger_softirq函数。
#+begin_src c
    static void trigger_softirq(void *data)  
    {  
        struct softnet_data *queue = data;  
    //调度napi可以看到依旧是backlog 这个napi结构体。  
        __napi_schedule(&queue->backlog);  
        __get_cpu_var(netdev_rx_stat).received_rps++;  
    }  
#+end_src

上面的函数都分析完毕了，剩下的就很简单了。

首先来看netif_rx如何被修改的，它被修改的很简单，首先是得到当前skb所应该被处理的cpu id，然后再通过比较这个cpu和当前正在处理的cpu id进行比较来做不同的处理。

#+begin_src c
    int netif_rx(struct sk_buff *skb)  
    {  
        int cpu;  
      
        /* if netpoll wants it, pretend we never saw it */  
        if (netpoll_rx(skb))  
            return NET_RX_DROP;  
      
        if (!skb->tstamp.tv64)  
            net_timestamp(skb);  
    //得到cpu id。  
        cpu = get_rps_cpu(skb->dev, skb);  
        if (cpu < 0)  
            cpu = smp_processor_id();  
    //通过cpu进行队列不同的处理  
        return enqueue_to_backlog(skb, cpu);  
    }  
#+end_src

然后是netif_receive_skb,这里patch将内核本身的这个函数改写为__netif_receive_skb。然后当返回值小于0,则说明不需要对队列进行处理，此时直接发送到3层。
#+begin_src c
    int netif_receive_skb(struct sk_buff *skb)  
    {  
        int cpu;  
      
        cpu = get_rps_cpu(skb->dev, skb);  
      
        if (cpu < 0)  
            return __netif_receive_skb(skb);  
        else  
            return enqueue_to_backlog(skb, cpu);  
    }  
#+end_src


最后来总结一下，可以看到input_pkt_queue是一个FIFO的队列，而且如果当qlen有值的时候，也就是在另外的cpu有数据包放到input_pkt_queue中，则当前cpu不会调度napi，而是将数据包放到input_pkt_queue中，然后等待trigger_softirq来调度napi。

因此这个patch完美的解决了软中断在多核下的均衡问题，并且没有由于是同一个连接会map到相同的cpu，并且input_pkt_queue的使用，因此乱序的问题也不会出现。
  
* tcp_timestamp引发的问题
  tcp_tw_recycle 见[[tcp_tw_recycle]]
    近来线上陆续出现了一些connect失败的问题，经过分析试验，最终确认和proc参数tcp_tw_recycle/tcp_timestamps相关；
1. 现象
    第一个现象：模块A通过NAT网关访问服务S成功，而模块B通过NAT网关访问服务S经常性出现connect失败，抓包发现：服务S端已经收到了syn包，但没有回复synack；另外，模块A关闭了tcp timestamp，而模块B开启了tcp timestamp；
    第二个现象：不同主机上的模块C（开启timestamp），通过NAT网关（1个出口ip）访问同一服务S，主机C1 connect成功，而主机C2 connect失败；

2. 分析
    根据现象上述问题明显和tcp timestmap有关；查看linux 2.6.32内核源码，发现tcp_tw_recycle/tcp_timestamps都开启的条件下，60s内同一源ip主机的socket connect请求中的timestamp必须是递增的。
    源码函数：tcp_v4_conn_request(),该函数是tcp层三次握手syn包的处理函数（服务端）；
    源码片段：
   #+begin_src c
		/* VJ's idea. We save last timestamp seen
		 * from the destination in peer table, when entering
		 * state TIME-WAIT, and check against it before
		 * accepting new connection request.
		 *
		 * If "isn" is not zero, this request hit alive
		 * timewait bucket, so that all the necessary checks
		 * are made in the function processing timewait state.
		 */
		if (tmp_opt.saw_tstamp &&
		    tcp_death_row.sysctl_tw_recycle &&
		    (dst = inet_csk_route_req(sk, req)) != NULL &&
		    (peer = rt_get_peer((struct rtable *)dst)) != NULL &&
		    peer->v4daddr == saddr) {
			if (xtime.tv_sec < peer->tcp_ts_stamp + TCP_PAWS_MSL &&
			    (s32)(peer->tcp_ts - req->ts_recent) >
							TCP_PAWS_WINDOW) {
				NET_INC_STATS_BH(LINUX_MIB_PAWSPASSIVEREJECTED);
				dst_release(dst);
				goto drop_and_free;
			}
		}
   #+end_src
        tmp_opt.saw_tstamp：该socket支持tcp_timestamp
        sysctl_tw_recycle：本机系统开启tcp_tw_recycle选项
        TCP_PAWS_MSL：60s，该条件判断表示该源ip的上次tcp通讯发生在60s内
        TCP_PAWS_WINDOW：1，该条件判断表示该源ip的上次tcp通讯的timestamp 大于 本次tcp

    分析：主机client1和client2通过NAT网关（1个ip地址）访问serverN，由于timestamp时间为系统启动到当前的时间，因此，client1和client2的timestamp不相同；根据上述syn包处理源码，在tcp_tw_recycle和tcp_timestamps同时开启的条件下，timestamp大的主机访问serverN成功，而timestmap小的主机访问失败；

    参数：/proc/sys/net/ipv4/tcp_timestamps - 控制timestamp选项开启/关闭
          /proc/sys/net/ipv4/tcp_tw_recycle - 减少timewait socket释放的超时时间

    如果客户端是NAT出来的，并且我们server端有打开tcp_tw_recycle ,并且time stamp也没有关闭，那么假设第一个连接进来，然后关闭，此时这个句柄处于time wait状态，然后很快(小于60秒)又一个客户端(相同的源地址，如果打开了xfrm还要相同的端口号)发一个syn包，此时linux内核就会认为这个数据包异常的，因此就会丢掉这个包,并发送rst。

而现在大部分的客户端都是NAT出来的，因此建议tw_recycle还是关闭,或者说server段关闭掉time stamp(/proc/sys/net/ipv4/tcp_timestamps).

3. 解决方法
    echo 0 > /proc/sys/net/ipv4/tcp_tw_recycle;
    tcp_tw_recycle默认是关闭的，有不少服务器，为了提高性能，开启了该选项；
    为了解决上述问题，个人建议关闭tcp_tw_recycle选项，而不是timestamp；因为 在tcp timestamp关闭的条件下，开启tcp_tw_recycle是不起作用的；而tcp timestamp可以独立开启并起作用。
    源码函数：  tcp_time_wait()
    源码片段：
   #+begin_src c
        if (tcp_death_row.sysctl_tw_recycle && tp->rx_opt.ts_recent_stamp)
            recycle_ok = icsk->icsk_af_ops->remember_stamp(sk);
        ......
       
        if (timeo < rto)
            timeo = rto;

        if (recycle_ok) {
            tw->tw_timeout = rto;
        } else {
            tw->tw_timeout = TCP_TIMEWAIT_LEN;
            if (state == TCP_TIME_WAIT)
                timeo = TCP_TIMEWAIT_LEN;
        }

        inet_twsk_schedule(tw, &tcp_death_row, timeo,
                   TCP_TIMEWAIT_LEN);
   #+end_src
    timestamp和tw_recycle同时开启的条件下，timewait状态socket释放的超时时间和rto相关；否则，超时时间为TCP_TIMEWAIT_LEN，即60s；

    内核说明文档 对该参数的介绍如下：
    tcp_tw_recycle - BOOLEAN
    Enable fast recycling TIME-WAIT sockets. Default value is 0.
    It should not be changed without advice/request of technical
    experts.
* tcp_tw_reuse
  复用处于TIMEWAIT状态的sock
  inet_hash_connect()->__inet_check_established()->tcp_twsk_unique()
#+begin_src c
int tcp_twsk_unique(struct sock *sk, struct sock *sktw, void *twp)
{
	const struct tcp_timewait_sock *tcptw = tcp_twsk(sktw);
	struct tcp_sock *tp = tcp_sk(sk);

	/* With PAWS, it is safe from the viewpoint
	   of data integrity. Even without PAWS it is safe provided sequence
	   spaces do not overlap i.e. at data rates <= 80Mbit/sec.

	   Actually, the idea is close to VJ's one, only timestamp cache is
	   held not per host, but per port pair and TW bucket is used as state
	   holder.

	   If TW bucket has been already destroyed we fall back to VJ's scheme
	   and use initial timestamp retrieved from peer table.
	 */
	if (tcptw->tw_ts_recent_stamp &&
	    (twp == NULL || (sysctl_tcp_tw_reuse &&
			     xtime.tv_sec - tcptw->tw_ts_recent_stamp > 1))) {
		tp->write_seq = tcptw->tw_snd_nxt + 65535 + 2;
		if (tp->write_seq == 0)
			tp->write_seq = 1;
		tp->rx_opt.ts_recent	   = tcptw->tw_ts_recent;
		tp->rx_opt.ts_recent_stamp = tcptw->tw_ts_recent_stamp;
		sock_hold(sktw);
		return 1;
	}

	return 0;
}
#+end_src
* tcp_tw_recycle
  快速回收处于TIMEWAIT状态的sock
  见inet_twsk_schedule()
* TCPIP函数调用大致流程

**  ip_send_reply 
    ip_route_output_key 
    ip_push_pending_frames 


** tcp_v4_send_ack(发送ACK) 
    ip_send_reply 
        ip_route_output_key 
        ip_push_pending_frames 


** 用户子上而下的读函数都间接的调用了tcp_recvmsg 
tcp_recvmsg
skb_copy_datagram_iovec 
tcp_recv_urg(接受一个字节的URG数据) 

** UDP
UDP的写函数都调用了udp_sendmsg 
udp_sendmsg
    ip_route_output_flow 
    ip_append_data 
    udp_flush_pending_frames 
        ip_flush_pending_frames 
    udp_push_pending_frames 
        ip_push_pending_frames 

接收
硬件->IP层->运输层收到数据，添加到对应的SOCKET缓冲区中 
由ip_rcv间接调用 
udp_rcv 
    __udp4_lib_rcv 
        if(是多播或广播) 
            __udp4_lib_mcast_deliver 
                udp_queue_rcv_skb(对每个需要接受的UDP SOCKET缓冲调用) 
        __udp4_lib_lookup 
        udp_queue_rcv_skb 


把数据块sk_buff放到一个sock结构的接受缓存的末尾中 
udp_queue_rcv_skb 
    sock_queue_rcv_skb 
        skb_queue_tail 

用户子上而下的读函数都间接的调用了udp_recvmsg 
udp_recvmsg
__skb_recv_datagram 
skb_copy_datagram_iovec 
skb_copy_and_csum_datagram_iovec 

** 原始套接字 
RAW Socket的写函数都调用了raw_sendmsg 
raw_sendmsg
    ip_route_output_flow 
    if(设置了IP_HDRINCL选项，即自己构造ip头部) 
        raw_send_hdrinc
    else 
        ip_append_data 
        ip_flush_pending_frames或 
        ip_push_pending_frames 


自底向上的收包 
raw_rcv 
由ip_forward调用ip_call_ra_chain，然后再调用的raw_rcv 
raw_rcv 
sock_queue_rcv_skb 
skb_queue_tail 
sk->sk_data_ready = sock_def_readable 
waitqueue_active 
sk_wake_async 


用户子上而下的读函数都间接的调用了raw_recvmsg 
raw_recvmsg
skb_recv_datagram 
__skb_recv_datagram 
wait_for_packet(如果没有数据，则调用此函数等待数据) 


** ICMP 
在任何需要发送ICMP报文的时候都会调用此函数 
icmp_send 
    __ip_route_output_key 
        ip_route_output_slow 
    ip_route_output_key 
        ip_route_output_flow 
    icmp_push_reply    
        ip_append_data 
        ip_flush_pending_frames或 
        ip_push_pending_frames 


硬件->IP层->运输层收到ICMP数据，作出处理逻辑 
由ip_rcv间接调用 
icmp_rcv 
    完全就是icmp协议的处理逻辑，通过函数指针icmp_pointers[icmph->type].handler调用了一下函数中的某一个 
    icmp_discard 
    icmp_unreach 
    icmp_redirect 
    icmp_timestamp 
    icmp_address 
    icmp_address_reply 
    icmp_echo 


** 网络层 
IP发送 
网络层中主要的发送函数有以下三个：ip_push_pending_frames，ip_queue_xmit，raw_send_hdrinc 
ip_push_pending_frames 
将所有pending状态的IP分组组合成一个IP分组，并发送 
    ip_local_out 


ip_queue_xmit 
    ip_route_output_flow(找路由) 
    ip_local_out 


raw_send_hdrinc 
    NF_HOOK(dst_output) 


ip_local_out 
    __ip_local_out 
        nf_hook(dst_output) 
    dst_output 

路由选择 
ip_route_output_flow 
    __ip_route_output_key 
        ip_route_output_slow 

路由选择 
ip_route_output_slow 
    fib_lookup 
    ip_mkroute_output 
        __mkroute_output 
        rt_hash 
        rt_intern_hash 
            arp_bind_neighbour 
                __neigh_lookup_errno 
                    neigh_lookup 
                    neigh_create 


dst_output 
    dst->output = ip_output 
    NF_HOOK_COND(ip_finish_output) 
        dst_output 
        ip_fragment 
        ip_finish_output2 
            neigh_hh_output 
                hh->hh_output = dev_queue_xmit 
            dst->neighbour->output = neigh_resolve_output 
                neigh->ops->queue_xmit = dev_queue_xmit 

IP接受 
接收IPv4包，由netif_rx间接调用 
ip_rcv 
    NF_HOOK 
    ip_rcv_finish 
        ip_route_input 
        dst_input 
            dst->input(可能是ip_local_deliver或ip_forward) 
            if(是发给本地的包) 
                dst->input是ip_local_deliver 
                    NF_HOOK 
                    ip_local_deliver_finish 
                    ipprot->handler(可能是tcp_v4_rcv,udp_rcv,icmp_rcv,igmp_rcv) 
            else 
                dst->input是ip_forward 

更新路由 
ip_route_input 
    ip_route_input_mc(多播) 
        rt_hash 
        rt_intern_hash 
    ip_route_input_slow(其它) 
        ip_mkroute_input 
            __mkroute_input 
            rt_hash 
            rt_intern_hash 
每收到一个IP报文都会调用此函数更新路由表。ip_route_input函数的上半部分是在hash table寻找路由项，如果找到就返回。找不到才会调用后面的ip_route_input_mc或ip_route_input_slow来更新路由表。 


转发 
ip_forward 
ip_call_ra_chain 
raw_rcv 
    xfrm4_route_forward(处理路由) 
        xfrm_route_forward 
            __xfrm_route_forward 
                xfrm_lookup 
                    __xfrm_lookup 
                        xfrm_find_bundle 
                            afinfo->find_bundle = __xfrm4_find_bundle 
                        xfrm_bundle_create 
                            xfrm_dst_lookup 
                                afinfo->dst_lookup = xfrm4_dst_lookup 
                                    __ip_route_output_key 
                                        ip_route_output_slow 
    处理各个参数(在一定条件下发送ICMP) 
    ip_decrease_ttl(减少TTL) 
    NF_HOOK(ip_forward_finish) 
        dst_output 


** 链路层 
接收帧 
由硬件驱动在中断处理程序中直接调用netif_rx 
netif_rx 
    if(netpoll_rx函数与把数据拿走) 
        return 
    __skb_queue_tail(把所有收到的数据保存起来) 
    napi_schedule 
        __napi_schedule 
            __raise_softirq_irqoff(NET_RX_SOFTIRQ); 

在net_dev_init函数中初始化了软中断： 
open_softirq(NET_TX_SOFTIRQ, net_tx_action, NULL); 
open_softirq(NET_RX_SOFTIRQ, net_rx_action, NULL); 
所以NET_RX_SOFTIRQ中断的处理函数是net_rx_action，NET_TX_SOFTIRQ中断的处理函数是net_tx_action。需要让上层接收数据时，只要触发相应的软中断，如__raise_softirq_irqoff(NET_RX_SOFTIRQ)。内核就会在适当时机执行do_softirq来处理pending的软中断。 


net_rx_action 
    n->poll = process_backlog 
        netif_receive_skb 
            pt_prev->func = ip_rcv(在这里完成了交接) 
    __raise_softirq_irqoff(NET_RX_SOFTIRQ) 


发送帧 
dev_queue_xmit 
    rcu_read_lock_bh 
    if(设备有发送队列) 
        q->enqueue(将数据追加到发送队列，软中断处理函数net_tx_action会执行真正的发送工作) 
    else 
        dev_hard_start_xmit 
            dev->hard_start_xmit = el_start_xmit 
                调用outw汇编指令发送数据，够底层了 
    rcu_read_unlock_bh 


net_tx_action 
    __kfree_skb(释放已发送的，此时中断由dev_kfree_skb_irq函数发起) 
    qdisc_run 
        __qdisc_run 
qdisc_restart 
                dev_hard_start_xmit 
            netif_schedule 
    netif_schedule 


netif_schedule 
    __netif_schedule 
        raise_softirq_irqoff(NET_TX_SOFTIRQ)  


* Loadavg浅述 
Loadavg浅述 
http://kernel.taobao.org/index.php/Loadavg%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90
cat /proc/loadavg可以看到当前系统的load 
$ cat /proc/loadavg 
0.01 0.02 0.05 2/317 26207 
前面三个值分别对应系统当前1分钟、5分钟、15分钟内的平均load。load用于反映当前系统的负载情况
，对于16核的系统，如果每个核上cpu利用率为30%，则在不存在uninterruptible进程的情况下，系统load应该维持在4.8左右。
对16核系统，如果load维持在16左右，在不存在uninterrptible进程的情况下，意味着系统CPU几乎不存在空闲状态，利用率接近于100%。
结合iowait、vmstat和loadavg可以分析出系统当前的整体负载，各部分负载分布情况。 

Loadavg读取 

在内核中/proc/loadavg是通过load_read_proc来读取相应数据，下面首先来看一下load_read_proc的实现： 
fs/proc/proc_misc.c
#+begin_src c
static int loadavg_read_proc(char *page, char **start, off_t off, 
                                 int count, int *eof, void *data) 
{ 
        int a, b, c; 
        int len; 

        a = avenrun[0] + (FIXED_1/200); 
        b = avenrun[1] + (FIXED_1/200); 
        c = avenrun[2] + (FIXED_1/200); 
        len = sprintf(page,"%d.%02d %d.%02d %d.%02d %ld/%d %d\n", 
                LOAD_INT(a), LOAD_FRAC(a), 
                LOAD_INT(b), LOAD_FRAC(b), 
                LOAD_INT(c), LOAD_FRAC(c), 
                nr_running(), nr_threads, last_pid); 
        return proc_calc_metrics(page, start, off, count, eof, len); 
}
#+end_src

几个宏定义如下： 
#+begin_src c
 #define FSHIFT          11              /* nr of bits of precision */ 
 #define FIXED_1         (1<<FSHIFT)     /* 1.0 as fixed-point */ 
 #define LOAD_INT(x) ((x) >> FSHIFT) 
 #define LOAD_FRAC(x) LOAD_INT(((x) & (FIXED_1-1)) * 100)
#+end_src

根据输出格式，LOAD_INT对应计算的是load的整数部分，LOAD_FRAC计算的是load的小数部分。 
将a=avenrun[0] + (FIXED_1/200）带入整数部分和小数部分计算可得： 
#+begin_example
LOAD_INT(a) = avenrun[0]/(2^11) + 1/200
LOAD_FRAC(a) = ((avenrun[0]%(2^11) + 2^11/200) * 100) / (2^11)
             = (((avenrun[0]%(2^11)) * 100 + 2^10) / (2^11)
             = ((avenrun[0]%(2^11) * 100) / (2^11) +  
#+end_example

由上述计算结果可以看出，FIXED_1/200在这里是用于小数部分第三位的四舍五入，由于小数部分只取前两位，第三位如果大于5，则进一位，否则直接舍去。 

临时变量a/b/c的低11位存放的为load的小数部分值，第11位开始的高位存放的为load整数部分。因此可以得到a=load(1min) * 2^11 
因此有: load(1min) * 2^11 = avenrun[0] + 2^11 / 200 
进而推导出： load(1min)=avenrun[0]/(2^11) + 1/200 
忽略用于小数部分第3位四舍五入的1/200，可以得到load(1min)=avenrun[0] / 2^11，即： 
avenrun[0] = load(1min) * 2^11 

avenrun是个陌生的量，这个变量是如何计算的，和系统运行进程、cpu之间的关系如何，在第二阶段进行分析。 

Loadavg和进程之间的关系 

内核将load的计算和load的查看进行了分离，avenrun就是用于连接load计算和load查看的桥梁。 
下面开始分析通过avenrun进一步分析系统load的计算。 
avenrun数组是在calc_load中进行更新 
kernel/timer.c
#+begin_src c
 /* 
 * calc_load - given tick count, update the avenrun load estimates. 
 * This is called while holding a write_lock on xtime_lock. 
 */ 
static inline void calc_load(unsigned long ticks) 
{ 
        unsigned long active_tasks; /* fixed-point */ 
        static int count = LOAD_FREQ;  
        count -= ticks; 
        if (count < 0) { 
                count += LOAD_FREQ; 
                active_tasks = count_active_tasks(); 
                CALC_LOAD(avenrun[0], EXP_1, active_tasks); 
                CALC_LOAD(avenrun[1], EXP_5, active_tasks); 
                CALC_LOAD(avenrun[2], EXP_15, active_tasks); 
        } 
}
static unsigned long count_active_tasks(void) 
{ 
        return nr_active() * FIXED_1; 
}
#+end_src
 #define LOAD_FREQ       (5*HZ)          /* 5 sec intervals */ 
 #define EXP_1           1884            /* 1/exp(5sec/1min) as fixed-point */ 
 #define EXP_5           2014            /* 1/exp(5sec/5min) */ 
 #define EXP_15          2037            /* 1/exp(5sec/15min) */


calc_load在每个tick都会执行一次，每个LOAD_FREQ（5s）周期执行一次avenrun的更新。 
active_tasks为系统中当前贡献load的task数nr_active乘于FIXED_1，用于计算avenrun。宏CALC_LOAD定义如下： 
 #define CALC_LOAD(load,exp,n) \ 
       load *= exp; \ 
       load += n*(FIXED_1-exp); \ 
       load >>= FSHIFT;


用avenrun(t-1)和avenrun(t)分别表示上一次计算的avenrun和本次计算的avenrun，则根据CALC_LOAD宏可以得到如下计算： 
avenrun(t)=(avenrun(t-1) * EXP_N + nr_active * FIXED_1*(FIXED_1 – EXP_N)) / FIXED_1
          = avenrun(t-1) + (nr_active*FIXED_1 – avenrun(t-1)) * (FIXED_1 -EXP_N) / FIXED_1


推导出： 
avenrun(t) – avenrun(t-1) = (nr_active*FIXED_1 – avenrun(t-1)) * (FIXED_1 – EXP_N) / FIXED_1


将第一阶段推导的结果代入上式，可得： 
(load(t) – load(t-1)) * FIXED_1 = (nr_active – load(t-1)) * (FIXED_1 – EXP_N)


进一步得到nr_active变化和load变化之间的关系式： 

load(t) – load(t-1) = (nr_active – load(t-1)) * (FIXED_1 – EXP_N) / FIXED_1 

这个式子可以反映的内容包含如下两点： 
1）当nr_active为常数时，load会不断的趋近于nr_active，趋近速率由快逐渐变缓 
2）nr_active的变化反映在load的变化上是被降级了的，系统突然间增加10个进程， 
1分钟load的变化每次只能够有不到1的增加（这个也就是权重的的分配）。 

另外也可以通过将式子简化为： 

load(t)= load(t-1) * EXP_N / FIXED_1 + nr_active * (1 - EXP_N/FIXED_1)  

这样可以更加直观的看出nr_active和历史load在当前load中的权重关系 （多谢任震宇大师的指出） 
#define EXP_1           1884            /* 1/exp(5sec/1min) as fixed-point */ 
#define EXP_5           2014            /* 1/exp(5sec/5min) */ 
#define EXP_15          2037            /* 1/exp(5sec/15min) */


1分钟、5分钟、15分钟对应的EXP_N值如上，随着EXP_N的增大，(FIXED_1 – EXP_N)/FIXED_1值就越小， 
这样nr_active的变化对整体load带来的影响就越小。对于一个nr_active波动较小的系统，load会 
不断的趋近于nr_active，最开始趋近比较快，随着相差值变小，趋近慢慢变缓，越接近时越缓慢，并最 
终达到nr_active。如下图所示： 
文件:load 1515.jpg 


也因此得到一个结论，load直接反应的是系统中的nr_active。 那么nr_active又包含哪些？ 如何去计算 
当前系统中的nr_active？ 这些就涉及到了nr_active的采样。 

Loadavg采样 

nr_active直接反映的是为系统贡献load的进程总数，这个总数在nr_active函数中计算： 
kernel/sched.c
#+begin_src c
unsigned long nr_active(void) 
{ 
        unsigned long i, running = 0, uninterruptible = 0; 

        for_each_online_cpu(i) { 
                running += cpu_rq(i)->nr_running; 
                uninterruptible += cpu_rq(i)->nr_uninterruptible; 
        } 

        if (unlikely((long)uninterruptible < 0)) 
                uninterruptible = 0; 

        return running + uninterruptible; 
}
#+end_src

 #define TASK_RUNNING            0 
 #define TASK_INTERRUPTIBLE      1 
 #define TASK_UNINTERRUPTIBLE    2 
 #define TASK_STOPPED            4 
 #define TASK_TRACED             8 
/* in tsk->exit_state */ 
 #define EXIT_ZOMBIE             16 
 #define EXIT_DEAD               32 
/* in tsk->state again */ 
 #define TASK_NONINTERACTIVE     64


该函数反映，为系统贡献load的进程主要包括两类，一类是TASK_RUNNING，一类是TASK_UNINTERRUPTIBLE。
 当5s采样周期到达时，对各个online-cpu的运行队列进行遍历，取得当前时刻该队列上running和uninterruptible的
 进程数作为当前cpu的load，各个cpu load的和即为本次采样得到的nr_active。 


18内核计算loadavg存在的问题 

xtime_lock解析 

内核在5s周期执行一次全局load的更新，这些都是在calc_load函数中执行。追寻calc_load的调用： 
kernel/timer.c
#+begin_src c
static inline void update_times(void) 
{  
        unsigned long ticks; 

        ticks = jiffies - wall_jiffies; 
        wall_jiffies += ticks; 
        update_wall_time(); 
        calc_load(ticks); 
}
#+end_src

update_times中更新系统wall time，然后执行全局load的更新。 
kernel/timer.c
#+begin_src c
void do_timer(struct pt_regs *regs) 
{  
        jiffies_64++; 
        /* prevent loading jiffies before storing new jiffies_64 value. */ 
        barrier(); 
        update_times(); 
}
#+end_src

do_timer中首先执行全局时钟jiffies的更新，然后是update_times。 
#+begin_src c
void main_timer_handler(struct pt_regs *regs) 
{ 
...
        write_seqlock(&xtime_lock);
...
        do_timer(regs); 
 #ifndef CONFIG_SMP 
        update_process_times(user_mode(regs)); 
 #endif 
...
        write_sequnlock(&xtime_lock); 
}
#+end_src

对wall_time和全局jiffies的更新都是在加串行锁（sequence lock）xtime_lock之后执行的。 
include/linux/seqlock.h
#+begin_src c
static inline void write_seqlock(seqlock_t *sl) 
{ 
        spin_lock(&sl->lock);
        ++sl->sequence; 
        smp_wmb(); 
} 

static inline void write_sequnlock(seqlock_t *sl) 
{ 
        smp_wmb(); 
        sl->sequence++; 
        spin_unlock(&sl->lock); 
} 
 
typedef struct { 
        unsigned sequence; 
        spinlock_t lock; 
} seqlock_t;
#+end_src

sequence lock内部保护一个用于计数的sequence。Sequence lock的写锁是通过spin_lock实现的， 
在spin_lock后对sequence计数器执行一次自增操作，然后在锁解除之前再次执行sequence的自增操作。 
sequence初始化时为0。这样，当锁内部的sequence为奇数时，说明当前该sequence lock的写锁正被拿， 
读和写可能不安全。如果在写的过程中，读是不安全的，那么就需要在读的时候等待写锁完成。对应读锁使用如下： 
#+begin_src c
 #if (BITS_PER_LONG < 64) 
u64 get_jiffies_64(void) 
{ 
        unsigned long seq; 
        u64 ret;  

        do { 
                seq = read_seqbegin(&xtime_lock); 
                ret = jiffies_64; 
        } while (read_seqretry(&xtime_lock, seq)); 
        return ret; 
} 

EXPORT_SYMBOL(get_jiffies_64); 
 #endif 
#+end_src

读锁实现如下： 
#+begin_src c
static __always_inline unsigned read_seqbegin(const seqlock_t *sl) 
{ 
        unsigned ret = sl->sequence; 
        smp_rmb(); 
        return ret; 
} 

static __always_inline int read_seqretry(const seqlock_t *sl, unsigned iv) 
{ 
        smp_rmb(); 
      	/*iv为读之前的锁计数器
        * 当iv为基数时，说明读的过程中写锁被拿，可能读到错误值
        * 当iv为偶数，但是读完之后锁的计数值和读之前不一致，则说明读的过程中写锁被拿，
        * 也可能读到错误值。
        */
        return (iv & 1) | (sl->sequence ^ iv);  
}
#+end_src

至此xtime_lock的实现解析完毕，由于对应写锁基于spin_lock实现，多个程序竞争写锁时等待者会一直循环等待， 
当锁里面处理时间过长，会导致整个系统的延时增长。另外，如果系统存在很多xtime_lock的读锁，在某个程 
序获取该写锁后，读锁就会进入类似spin_lock的循环查询状态，直到保证可以读取到正确值。因此需要尽可能 
短的减少在xtime_lock写锁之间执行的处理流程。 

 全局load读写分离解xtime_lock问题 

在计算全局load函数calc_load中，每5s需要遍历一次所有cpu的运行队列，获取对应cpu上的load。1）由于cpu个数是不固 
定的，造成calc_load的执行时间不固定，在核数特别多的情况下会造成xtime_lock获取的时间过长。2）calc_load是 
每5s一次的采样程序，本身并不能够精度特别高，对全局avenrun的读和写之间也不需要专门的锁保护，可以将全局load的 
更新和读进行分离。 
Dimitri Sivanich提出在他们的large SMP系统上，由于calc_load需要遍历所有online CPU，造成系统延迟较大。 
基于上述原因Thomas Gleixnert提交了下述patch对该bug进行修复： 
[Patch 1/2] sched, timers: move calc_load() to scheduler
[Patch 2/2] sched, timers: cleanup avenrun users


文件:rw isolate.jpg 

Thomas的两个patch，主要思想如上图所示。首先将全局load的计算分离到per-cpu上，各个cpu上计算load时不加xtime_lock 
的锁，计算的load更新到全局calc_load_tasks中，所有cpu上load计算完后calc_load_tasks即为整体的load。在5s定 
时器到达时执行calc_global_load，读取全局cacl_load_tasks，更新avenrun。由于只是简单的读取calc_load_tasks， 
执行时间和cpu个数没有关系。 

 几个关键点： 

 不加xtime_lock的per cpu load计算 

在不加xtime_lock的情况下，如何保证每次更新avenrun时候读取的calc_load_tasks为所有cpu已经更新之后的load？ 

Thomas的解决方案 

Thomas的做法是将定时器放到sched_tick中，每个cpu都设置一个LOAD_FREQ定时器。 
定时周期到达时执行当前处理器上load的计算。sched_tick在每个tick到达时执行 
一次，tick到达是由硬件进行控制的，客观上不受系统运行状况的影响。 

sched_tick的时机 

将per-cpu load的计算放至sched_tick中执行，第一反应这不是又回到了时间处理中断之间，是否依旧 
存在xtime_lock问题？ 下面对sched_tick进行分析（以下分析基于linux-2.6.32-220.17.1.el5源码） 
#+begin_src c
static void update_cpu_load_active(struct rq *this_rq) 
{ 
        update_cpu_load(this_rq); 

        calc_load_account_active(this_rq); 
}
 
void scheduler_tick(void) 
{ 
        int cpu = smp_processor_id(); 
        struct rq *rq = cpu_rq(cpu); 
...
        spin_lock(&rq->lock); 
...
        update_cpu_load_active(rq); 
...
        spin_unlock(&rq->lock); 

...
} 
 
void update_process_times(int user_tick) 
{ 
...
        scheduler_tick(); 
...
}
 
static void tick_periodic(int cpu) 
{ 
        if (tick_do_timer_cpu == cpu) { 
                write_seqlock(&xtime_lock); 

                /* Keep track of the next tick event */ 
                tick_next_period = ktime_add(tick_next_period, tick_period); 
           
                do_timer(1);  // calc_global_load在do_timer中被调用
                write_sequnlock(&xtime_lock); 
        } 
 
        update_process_times(user_mode(get_irq_regs())); 
...
}
 
void tick_handle_periodic(struct clock_event_device *dev) 
{ 
        int cpu = smp_processor_id(); 
...
        tick_periodic(cpu); 
...
}
#+end_src

交错的时间差 

将per-cpu load的计算放到sched_tick中后，还存在一个问题就是何时执行per-cpu上的load计算，如何保证更新全 
局avenrun时读取的全局load为所有cpu都计算之后的？ 当前的方法是给所有cpu设定同样的步进时间LOAD_FREQ， 
过了这个周期点当有tick到达则执行该cpu上load的计算，更新至全局的calc_load_tasks。calc_global_load 
的执行点为LOAD_FREQ+10，即在所有cpu load计算执行完10 ticks之后，读取全局的calc_load_tasks更新avenrun。 



通过将calc_global_load和per-cpu load计算的时间进行交错，可以避免calc_global_load在各个cpu load计算之间执行， 
导致load采样不准确问题。 

32内核Load计数nohz问题 

一个问题的解决，往往伴随着无数其他问题的诞生！Per-cpu load的计算能够很好的分离全局load的更新和读取，避免大型系统中cpu 
核数过多导致的xtime_lock问题。但是也同时带来了很多其他需要解决的问题。这其中最主要的问题就是nohz问题。 

为避免cpu空闲状态时大量无意义的时钟中断，引入了nohz技术。在这种技术下，cpu进入空闲状态之后会关闭该cpu对应的时钟中断，等 
到下一个定时器到达，或者该cpu需要执行重新调度时再重新开启时钟中断。 

cpu进入nohz状态后该cpu上的时钟tick停止，导致sched_tick并非每个tick都会执行一次。这使得将per-cpu的load计算放在 
sched_tick中并不能保证每个LOAD_FREQ都执行一次。如果在执行per-cpu load计算时，当前cpu处于nohz状态，那么当 
前cpu上的sched_tick就会错过，进而错过这次load的更新，最终全局的load计算不准确。 
基于Thomas第一个patch的思想，可以在cpu调度idle时对nohz情况进行处理。采用的方式是在当前cpu进入idle前进行一次该cpu 
上load的更新，这样即便进入了nohz状态，该cpu上的load也已经更新至最新状态，不会出现不更新的情况。如下图所示： 



理论上，该方案很好的解决了nohz状态导致全局load计数可能不准确的问题，事实上这却是一个苦果的开始。大量线上应用反馈 
最新内核的load计数存在问题，在16核机器cpu利用率平均为20%~30%的情况下，整体load却始终低于1。 

 解决方案 

接到我们线上报告load计数偏低的问题之后，进行了研究。最初怀疑对全局load计数更新存在竞争。对16核的系统，如果都没有进入 
nohz状态，那么这16个核都将在LOAD_FREQ周期到达的那个tick内执行per-cpu load的计算，并更新到全局的load中，这 
之间如果存在竞争，整体计算的load就会出错。当前每个cpu对应rq都维护着该cpu上一次计算的load值，如果发现本次计算load 
和上一次维护的load值之间差值为0，则不用更新全局load，否则将差值更新到全局load中。正是由于这个机制，全局load如果被 
篡改，那么在各个cpu维护着自己load的情况下，全局load最终将可能出现负值。而负值通过各种观察，并没有在线上出现，最终竞 
争条件被排除。 

通过/proc/sched_debug对线上调度信息进行分析，发现每个时刻在cpu上运行的进程基本维持在2~3个，每个时刻运行有进程的cpu都 
不一样。进一步分析，每个cpu上平均每秒出现sched_goidle的情况大概为1000次左右。因此得到线上每次进入idle的间隔为1ms/次。 
结合1HZ=1s=1000ticks，可以得到1tick =1ms。所以可以得到线上应用基本每一个tick就会进入一次idle！！！ 这个发现就好比 
原来一直用肉眼看一滴水，看着那么完美那么纯净，突然间给你眼前架了一个放大镜，一下出现各种凌乱的杂碎物。 在原有的世界里， 
10ticks是那么的短暂，一个进程都可能没有运行完成，如今发现10ticks内调度idle的次数就会有近10次。接着用例子对应用场景进行分析： 





(说明：可能你注意到了在5HZ+5到5HZ+11过程中也有CPU从非idle进入了idle，但是为什么没有-1，这里是由于每个cpu都保留 
了一份该CPU上一次计算时的load，如果load没有变化则不进行计算，这几个cpu上一次计算load为0，并没有变化) 

Orz！load为3的情况直接算成了0，难怪系统整体load会偏低。这里面的一个关键点是：对已经计算过load的cpu，我们对idle进 
行了计算，却从未考虑过这给从idle进入非idle的情况带来的不公平性。这个是当前线上2.6.32系统存在的问题。在定位到问题 
之后，跟进到upstream中发现Peter Z针对该load计数问题先后提交了三个patch，最新的一个patch是在4月份提交。这三个 
patch如下： 
[Patch] sched: Cure load average vs NO_HZ woes
[Patch] sched: Cure more NO_HZ load average woes
[Patch] sched: Fix nohz load accounting – again!


这是目前我们backport的patch，基本思想是将进入idle造成的load变化暂时记录起来，不是每次进入idle都导致全局load的更新。 
这里面的难点是什么时候将idle更新至全局的load中？在最开始计算per-cpu load的时候需要将之前所有的idle都计算进来， 
由于目前各个CPU执行load计算的先后顺序暂时没有定，所以将这个计算放在每个cpu里面都计算一遍是一种方法。接着用示例进行说明： 


至此这三个patch能够很好的处理我们的之前碰到的进入idle的问题。 
将上述三个patch整理完后，在淘客前端线上机器中进行测试，测试结果表明load得到了明显改善。 

 更细粒度的时间问题 
* tcp keepalive 报文序号

#+begin_src c
/* This routine sends a packet with an out of date sequence
 * number. It assumes the other end will try to ack it.
 *
 * Question: what should we make while urgent mode?
 * 4.4BSD forces sending single byte of data. We cannot send
 * out of window data, because we have SND.NXT==SND.MAX...
 *
 * Current solution: to send TWO zero-length segments in urgent mode:
 * one is with SEG.SEQ=SND.UNA to deliver urgent pointer, another is
 * out-of-date with SND.UNA-1 to probe window.
 */
static int tcp_xmit_probe_skb(struct sock *sk, int urgent)
{
	struct tcp_sock *tp = tcp_sk(sk);
	struct sk_buff *skb;

	/* We don't queue it, tcp_transmit_skb() sets ownership. */
	skb = alloc_skb(MAX_TCP_HEADER, sk_allocation(sk, GFP_ATOMIC));
	if (skb == NULL)
		return -1;

	/* Reserve space for headers and set control bits. */
	skb_reserve(skb, MAX_TCP_HEADER);
	/* Use a previous sequence.  This should cause the other
	 * end to send an ack.  Don't queue or clone SKB, just
	 * send it.
	 */
	tcp_init_nondata_skb(skb, tp->snd_una - !urgent, TCPHDR_ACK);
	TCP_SKB_CB(skb)->when = tcp_time_stamp;
	return tcp_transmit_skb(sk, skb, 0, GFP_ATOMIC);
}

/* Initiate keepalive or window probe from timer. */
int tcp_write_wakeup(struct sock *sk)
{
  ...
	} else {
		if (between(tp->snd_up, tp->snd_una + 1, tp->snd_una + 0xFFFF))
			tcp_xmit_probe_skb(sk, 1);
        // 发送探测报文
		return tcp_xmit_probe_skb(sk, 0);
	}
}
#+end_src

tcp_keepalive_timer -> tcp_write_wakeup
keepalive报文序号是snd_una - 1，也就是上次发送报文中最后一个合法的序号。
按tcpdump 显示方式 上个报文 1:16(15)
那么keepalive检测报文的序号为15
* 连接丢失问题
服务A 与 服务B，之间进项压力测试，
测试两天下来发现，服务B上存在大量的处于ESTABLISH状态的连接。
而A上确没有发现对应的链接。

开始时推断是服务A与服务B之间存在网络丢包，导致丢失FIN报文。

后来换了测试环境采用同一网段的两台服务器，发现还是出现问题。

问题原因是：
服务B处理请求速度过慢，导致大量链接积压到半链接队列中，
服务A发送请求，等待回应超时，关闭链接

服务B的由于对应链接仍在半链接队列中，不接受FIN报文，所以丢弃该报文。
当B有机会将该链接accept到，使用该链接，这时实际对端的连接已经关闭了
* procps
  top ps vmstat等命令
* ftrace 使用
** 简介

ftrace是内建于Linux内核的跟踪工具，从2.6.27开始加入主流内核。ftrace的作用是帮助开发人员了解
Linux 内核的运行时行为，以便进行故障调试或性能分析。使用ftrace可以对内核函数调用、上下文切换进行跟踪，还可以查看中断被关闭的时长，跟踪内核态中的延迟以及性能问题等。使用ftrace对内核进行跟踪调试，可以找到内核中出现的问题的根源，通过ftrace来观察内核中发生的活动，则可以了解内核的工作机制。
** 原理

ftrace本质上是一种静态代码插装技术，借助于GCC提供的profile特性，在所有的内核函数的开始部分插入一段代码，通过插入的代码来实现跟踪功能。GCC的-pg选项将在每个函数入口处加入对 mcount 的调用代码，ftrace使用自己定义的mcount函数来代替默认的mcount，实现了对于函数调用的跟踪。在每个内核函数入口加入跟踪代码，必然会影响内核的性能，为了减小对内核性能的影响，ftrace支持动态跟踪功能。在没有启用其跟踪功能时，它将mcount换成nop指令，只有在开启跟踪功能时，才将mcount换成用户注册的跟踪函数，因此在不进行跟踪的时候，对内核性能的影响微乎其微。
** 使用

ftrace通过debugfs向用户态提供访问接口，使用时需先挂载debugfs，挂载debugfs之后，在挂载目录下（默认是/sys/kernel/debug）会有一个tracing的文件夹，这个文件夹就是ftrace导出的借口。该文件夹下一些主要的文件的用途如下：
#+CAPTION: 功能
| README文件     | 提供了一个简短的使用说明，展示了 ftrace 的操作命令序列。可以通过 cat 命令查看该文件以了解概要的操作流程 |
| current_tracer | 用于设置或显示当前使用的跟踪器，其缺省值为nop，即不做任何跟踪操作。                                     |
|available_tracers|记录了当前编译进内核的跟踪器的列表，current_tracer必须在该文件列出的跟踪器名字列表中。|
|trace文件|提供了查看获取到的跟踪信息的接口。可以通过 cat 等命令查看该文件以查看跟踪到的内核活动记录，也可以将其内容保存为记录文件以备后续查看|
|tracing_enabled|用于控制current_tracer中的跟踪器是否跟踪内核函数的调用情况。写入0会关闭跟踪活动，写入1则激活跟踪功能。|
|set_graph_function|设置要清晰显示调用关系的函数，显示的信息结构类似于 C 语言代码，这样在分析内核运作流程时会更加直观一些。在使用 function_graph 跟踪器时使用；缺省为对所有函数都生成调用关系序列，可以通过写该文件来指定需要特别关注的函数|
|buffer_size_kb|用于设置单个 CPU 所使用的跟踪缓存的大小|
|available_filter_functions|记录了当前可以跟踪的内核函数。对于不在该文件中列出的函数，无法跟踪其活动。|
|set_ftrace_filter| 用于指定要跟踪的函数，支持简单形式的含有通配符的表达式，这样可以用一个表达式一次指定多个目标函数；注意，要写入文件的函数名必须可以在文件 available_filter_functions中看到|
|set_ftrace_notrace |与set_ftrace_filter相反，用于指定不跟踪的函数|

Ftrace使用起来很简单，大致可以分成三步完成，首先是选择函数跟踪器、设置将要跟踪的函数、启用其跟踪功能，然后执行你想要跟踪的代码，执行完成之后，停止跟踪器，将结果导出。下面通过一些实例来看看ftrace的使用

*function跟踪器*
#+begin_example
# cd /sys/kernel/debug/tracing
# echo 1 > /proc/sys/kernel/ftrace_enabled
# echo function > current_tracer
# echo do_fork > set_ftrace_filter
# echo 1 > tracing_on
#+end_example
让内核执行一段时间
# echo 0 > tracing_on
# cat trace

*sched_switch 跟踪器*
#+begin_example
# cd /sys/kernel/debug/tracing
# echo 1 >/proc/sys/kernel/ftrace_enabled
# echo sched_switch >current_tracer
# echo 1 > tracing_on
#+end_example
让内核执行一段时间
# echo 0 > tracing_on
# cat trace

其他的跟踪器的使用也类似上面的流程。上面的过程很容易写成脚步，此外还可以通过trace-cmd来操作。

* 使用vm.dirty_radio和vm.dirty_background_ratio 调整磁盘缓存和性能
in previous posts on vm.swappiness and using RAM disks we talked about how the memory on a Linux guest is used for the OS itself (the kernel, buffers, etc.), applications, and also for file cache. File caching is an important performance improvement, and read caching is a clear win in most cases, balanced against applications using the RAM directly. Write caching is trickier. The Linux kernel stages disk writes into cache, and over time asynchronously flushes them to disk. This has a nice effect of speeding disk I/O but it is risky. When data isn’t written to disk there is an increased chance of losing it.

当大量IO发生时，可能写满缓存。
There is also the chance that a lot of I/O will overwhelm the cache, too. Ever written a lot of data to disk all at once, and seen large pauses on the system while it tries to deal with all that data? Those pauses are a result of the cache deciding that there’s too much data to be written asynchronously (as a non-blocking background operation, letting the application process continue), and switches to writing synchronously (blocking and making the process wait until the I/O is committed to disk). Of course, a filesystem also has to preserve write order, so when it starts writing synchronously it first has to destage the cache. Hence the long pause.


好消息是有几个可以调整的选项，我们可以根据实际的负载，做调整。
#+begin_example
$ sysctl -a | grep dirty
vm.dirty_background_ratio = 10
vm.dirty_background_bytes = 0
vm.dirty_ratio = 20
vm.dirty_bytes = 0
vm.dirty_writeback_centisecs = 500
vm.dirty_expire_centisecs = 3000
#+end_example
vm.dirty_background_ratio is the percentage of system memory that can be filled with “dirty” pages — memory pages that still need to be written to disk — before the pdflush/flush/kdmflush background processes kick in to write it to disk. My example is 10%, so if my virtual server has 32 GB of memory that’s 3.2 GB of data that can be sitting in RAM before something is done.

vm.dirty_ratio is the absolute maximum amount of system memory that can be filled with dirty pages before everything must get committed to disk. When the system gets to this point all new I/O blocks until dirty pages have been written to disk. This is often the source of long I/O pauses, but is a safeguard against too much data being cached unsafely in memory.

vm.dirty_background_bytes and vm.dirty_bytes are another way to specify these parameters. If you set the _bytes version the _ratio version will become 0, and vice-versa.

vm.dirty_expire_centisecs is how long something can be in cache before it needs to be written. In this case it’s 30 seconds. When the pdflush/flush/kdmflush processes kick in they will check to see how old a dirty page is, and if it’s older than this value it’ll be written asynchronously to disk. Since holding a dirty page in memory is unsafe this is also a safeguard against data loss.

vm.dirty_writeback_centisecs is how often the pdflush/flush/kdmflush processes wake up and check to see if work needs to be done.

You can also see statistics on the page cache in /proc/vmstat:
#+begin_example
$ cat /proc/vmstat | egrep "dirty|writeback"
 nr_dirty 878 nr_writeback 0 nr_writeback_temp 0
#+end_example

In my case I have 878 dirty pages waiting to be written to disk.
** 减少缓存
计算机领域中，你做什么调整，取决于你要干什么。
如果我们有高速磁盘系统，且磁盘系统有带电池的NVRAM缓存，那么我们把数据保留在OS的page cache是有风险的。
应该让数据尽快写入磁盘系统。这是我们减少下面两个选项：
vm.dirty_background_ratio = 5
vm.dirty_ratio = 10

** 增加缓存

还有一些场景缓存可以极大的提高性能。保存在LINUX上的数据不是十分关键和可以允许丢失，而且可能存在应用频繁的写同一个文件。
理论上
There are scenarios where raising the cache dramatically has positive effects on performance. These situations are where the data contained on a Linux guest isn’t critical and can be lost, and usually where an application is writing to the same files repeatedly or in repeatable bursts. In theory, by allowing more dirty pages to exist in memory you’ll rewrite the same blocks over and over in cache, and just need to do one write every so often to the actual disk. To do this we raise the parameters:

vm.dirty_background_ratio = 50 vm.dirty_ratio = 80

Sometimes folks also increase the vm.dirty_expire_centisecs parameter to allow more time in cache. Beyond the increased risk of data loss, you also run the risk of long I/O pauses if that cache gets full and needs to destage, because on large VMs there will be a lot of data in cache.

** 两者都用

There are also scenarios where a system has to deal with infrequent, bursty traffic to slow disk (batch jobs at the top of the hour, midnight, writing to an SD card on a Raspberry Pi, etc.). In that case an approach might be to allow all that write I/O to be deposited in the cache so that the background flush operations can deal with it asynchronously over time:

vm.dirty_background_ratio = 5 vm.dirty_ratio = 80

Here the background processes will start writing right away when it hits that 5% ceiling but the system won’t force synchronous I/O until it gets to 80% full. From there you just size your system RAM and vm.dirty_ratio to be able to consume all the written data. Again, there are tradeoffs with data consistency on disk, which translates into risk to data. Buy a UPS and make sure you can destage cache before the UPS runs out of power. :)

No matter the route you choose you should always be gathering hard data to support your changes and help you determine if you are improving things or making them worse. In this case you can get data from many different places, including the application itself, /proc/vmstat, /proc/meminfo, iostat, vmstat, and many of the things in /proc/sys/vm. Good luck!
* LXC虚拟机操作系统日志混乱问题

** 解决办法
登陆到每个LXC虚机，进行如下操作
1.修改配置/etc/init.d/syslog:
将其中
start_klogd=yes
修改为：
start_klogd=no
 
2.执行如下命令，重启syslog服务
service syslog restart 

** 问题原因
   
   上层应用程序通过/proc/kmsg来获取操作系统的日志信息。
   这些日志信息由printk调用输出到ring buffer中
   读取/proc/kmsg就是读取该ring buffer的内容。
   当有多个klog进程同时读取/proc/kmsg时，就会出现每个进程都读不到完整的日志

   每个LXC虚拟机都会启动一个syslog服务，而syslog依赖klog。
   最后在物理机层面上会出现多个klog进程

代码见/kernel/printk.c

* LVS "IPV6 header not found"
  lvs服务器内核报错：IPv6 header not found
** 问题根源
在于kernel 3.10代码ip_vs_core.c中：
#+begin_src c
	/* Before ip_vs_in, change source only for VS/NAT */
	{
		.hook		= ip_vs_local_reply6,
		.owner		= THIS_MODULE,
		.pf		= NFPROTO_IPV4,
		.hooknum	= NF_INET_LOCAL_OUT,
		.priority	= NF_IP6_PRI_NAT_DST + 1,
	},
	/* After mangle, schedule and forward local requests */
	{
		.hook		= ip_vs_local_request6,
		.owner		= THIS_MODULE,
		.pf		= NFPROTO_IPV6,
		.hooknum	= NF_INET_LOCAL_OUT,
		.priority	= NF_IP6_PRI_NAT_DST + 2,
	},
#+end_src
NFPROTO_IPV4应为NFPROTO_IPV6
ip_vs_local_reply6 被错挂到ipv4的处理流程上。
所以本机发送的消息，可能被改错误的hook function处理。
（LVS本身发送的信息，在这些hook函数入口，直接返回了，不受影响）
 
现在发送一个报文：
ip_vs_local_reply6 -> ip_vs_out
ip_vs_out部分代码如下：（这部分代码只有非LVS报文才能走到）
#+begin_src c
	ip_vs_fill_iph_skb(af, skb, &iph);
#ifdef CONFIG_IP_VS_IPV6
	if (af == AF_INET6) {
		if (unlikely(iph.protocol == IPPROTO_ICMPV6)) {
			int related;
			int verdict = ip_vs_out_icmp_v6(skb, &related,
							hooknum, &iph);

			if (related)
				return verdict;
		}
	} else
#endif
#+end_src
ip_vs_fill_iph_skb调用ipv6_find_hdr 设置iphdr->protocol
#+begin_src c
static inline void
ip_vs_fill_iph_skb(int af, const struct sk_buff *skb, struct ip_vs_iphdr *iphdr)
{
#ifdef CONFIG_IP_VS_IPV6
	if (af == AF_INET6) {
		const struct ipv6hdr *iph =
			(struct ipv6hdr *)skb_network_header(skb);
		iphdr->saddr.in6 = iph->saddr;
		iphdr->daddr.in6 = iph->daddr;
		/* ipv6_find_hdr() updates len, flags */
		iphdr->len	 = 0;
		iphdr->flags	 = 0;
		iphdr->protocol  = ipv6_find_hdr(skb, &iphdr->len, -1,
						 &iphdr->fragoffs,
						 &iphdr->flags);
	} else
#endif
#+end_src

当某个ipv4报文第40个字节恰好等于58，也就是IPPROTO_ICMPV6时。
ipv6_find_hdr很可能把该报文的协议解释为合法的ICMP6。
这时，在图1中iph.protocol == IPPROTO_ICMPV6就判断通过了。
接下来会执行ip_vs_out_icmp6_v6 -> ipv6_find_hdr 
ip_vs_out_icmp6 代码如下：
#+begin_src c
    //对于非icmp报，或者其他长度少于icmp包，就被直接drop了
	ic = frag_safe_skb_hp(skb, ipvsh->len, sizeof(_icmph), &_icmph, ipvsh);
	if (ic == NULL)
		return NF_DROP;
    ...
	/* skip possible IPv6 exthdrs of contained IPv6 packet */
	ciph.protocol = ipv6_find_hdr(skb, &ciph.len, -1, &ciph.fragoffs, NULL);
	if (ciph.protocol < 0)
		return NF_ACCEPT; /* Contained IPv6 hdr looks wrong, ignore */
#+end_src
注意：到ciph.protocol < 0 时， 返回ACCEPT，也就是该报文最后会走到正常的流程。
 
再第二次调用ipv6_find_hdr，下图中画红线代码判断发现问题，于是打印出了IPv6 header not found
ipv6_find_hdr:
#+begin_src c
	if (*offset) {
		struct ipv6hdr _ip6, *ip6;

		ip6 = skb_header_pointer(skb, *offset, sizeof(_ip6), &_ip6);
		if (!ip6 || (ip6->version != 6)) {
			printk(KERN_ERR "IPv6 header not found\n");
			return -EBADMSG;
		}
		start = *offset + sizeof(struct ipv6hdr);
		nexthdr = ip6->nexthdr;
	}
 #+end_src
** 触发原因
日志上报和应用商店某个接口，会发送大量数据到服务端，不是典型的入口流量小出口流量多的场景。
会出现一些TCP协议的IP报大小超过1480的情况，例如1492字节，当出现这些打包时，由于我们的LVS采用了tunnel，也就IP over IP的方式。
当向realserver转发数据时，1492+20 = 1512 > MTU(1500)，相关判断在下图__ip_vs_get_out_rt中
#+begin_src c
 	if (likely(!(rt_mode & IP_VS_RT_MODE_TUNNEL))) {
		mtu = dst_mtu(&rt->dst);
		df = iph->frag_off & htons(IP_DF);
	} else {
		struct sock *sk = skb->sk;

		mtu = dst_mtu(&rt->dst) - sizeof(struct iphdr);
		if (mtu < 68) {
			IP_VS_DBG_RL("%s(): mtu less than 68\n", __func__);
			goto err_put;
		}
		ort = skb_rtable(skb);
		if (!skb->dev && sk && sk->sk_state != TCP_TIME_WAIT)
			ort->dst.ops->update_pmtu(&ort->dst, sk, NULL, mtu);
		/* MTU check allowed? */
		df = sysctl_pmtu_disc(ipvs) ? iph->frag_off & htons(IP_DF) : 0;
	}

	/* MTU checking */
	if (unlikely(df && skb->len > mtu && !skb_is_gso(skb))) {
		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_FRAG_NEEDED, htonl(mtu));
		IP_VS_DBG(1, "frag needed for %pI4\n", &iph->saddr);
		goto err_put;
	}
 #+end_src
当出现这种情况时，icmp_send向终端发送ICMP包通知对方：报文过大。
但是当终端的IP是58.x.x.x时，icmp报文中58的位置刚好和ipv6中protocol位置一样，所以这种发往58.x.x.x的这种icmp报文可能触发LVS bug。
 
** 问题影响
经过上面分析，我们可以发现实际上，有两个问题。
1、LVS tunnel模式：虽然Director的MTU是1500，实际上只能最大接受1480的报文
    问题1，是一直存在的问题，对性能略微有影响，TCP路径MTU发现时，会接受到icmp包的通知，获取实际MTU 1480，后续通讯时，不会再发送超过1480的报文。
   可以不与处理。
 
2、LVS BUG 本身   
     a. 发送到58.x.x.x网端的ICMP包会触发LVS bug。打印告警日志。
       但是实际上不影响该ICMP报文的发送。
      (即使影响的ICMP发送，由于TCP会重传，且重传时会使用默认MSS 576，长度不会超过1480)
     可以不与处理。
 
    b. 导致本机其他应用对外发送消息存在一定概率的丢失问题。
       在LVS director上，只存在ipvsman在发送检测信息且一直工作正常，
      只有UDP第11字节是58的报文，和 TCP第0字节是58的报文，才有可能被拦截。
      58是“：”的ascii码。ipvsman发送报文不会出现这种情况。
 
    当中间的网络设备（运营商的路由器和交换机的MTU少于1500时），上面问题1和问题2.a都不会发生。

* kprobe使用
Kprobe is a very simple method to probe the running kernel. At a fundamental level, it requires the address of a kernel function that needs to be debugged. Then, you create pre- and post-handlers that will print a debugging message when the target kernel function is called. (Actually, a handler performs any action specified in its code; in this case, it happens to be printing.) Thus, every time that function is called, you can track it.
An example

To keep things simple, I have created a small and easy-to-understand example. The target kernel function is ip_rcv(). The Kprobe example kernel module is as follows:
mod1.c
#+begin_src c	
#include<linux/module.h>
#include<linux/version.h>
#include<linux/kernel.h>
#include<linux/init.h>
#include<linux/kprobes.h>
 
static unsigned int counter = 0;
int Pre_Handler(struct kprobe *p, struct pt_regs *regs){
    printk("Pre_Handler: counter=%u\n",counter++);
    return 0;
}
 
void Post_Handler(struct kprobe *p, struct pt_regs *regs, unsigned long flags){
    printk("Post_Handler: counter=%u\n",counter++);
}
 
static struct kprobe kp;
 
int myinit(void)
{
    printk("module inserted\n ");
    kp.pre_handler = Pre_Handler;
    kp.post_handler = Post_Handler;
    kp.addr = (kprobe_opcode_t *)0xc071c9a9;
    register_kprobe(&kp);
    return 0;
}
 
void myexit(void)
{
    unregister_kprobe(&kp);
    printk("module removed\n ");
}
 
module_init(myinit);
module_exit(myexit);
MODULE_AUTHOR("Manoj");
MODULE_DESCRIPTION("KPROBE MODULE");
MODULE_LICENSE("GPL");
#+end_src

The makefile required to build the kernel module object file that you need to insert into the kernel is as follows:
#+begin_src c
obj-m +=mod1.o
KDIR= /lib/modules/$(shell uname -r)/build
all:
    $(MAKE) -C $(KDIR) SUBDIRS=$(PWD) modules
clean:
       rm -rf *.o *.ko *.mod.* .c* .t*
#+end_src
Code walk-through

Here’s an explanation for the less obvious sections of the code.
struct kprobe kp;

To make use of Kprobe functionality, you must declare a variable of the structure struct kprobe, which is declared in include/linux/kprobes.h. Here’s a little extract:
#+begin_src c
struct kprobe {
    .
    .
    kprobe_opcode_t *addr;
    kprobe_pre_handler_t pre_handler;
    kprobe_post_handler_t post_handler;
}
#+end_src
The three members listed above are of interest to us. You need to assign the kernel address of the target function to the addr member; you can retrieve the address from the /proc/kallsyms file, as follows:
#+begin_src c
# cat /proc/kallsyms | grep ip_rcv
c071c3e0 t ip_rcv_finish
c071c9a9 T ip_rcv
#+end_src
Once you’ve found the address, use it in the myinit() function, as follows:
kp.addr = (kprobe_opcode_t *)0xc071c9a9;

Kprobe executes handler functions before and after the target kernel function is called, and we created the Pre_Handler() and Post_Handler() functions for this purpose. Assign these to their respective pointer members in the Kprobe struct — pre_handler and post_handler — in myinit(), as you can see. Finally, register your Kprobe with the kernel, with register_kprobe(&kp);.

Then compile the module by running make:
#+begin_example
# make
make -C /lib/modules/2.6.34/build SUBDIRS=/root/kprobe modules
make[1]: Entering directory '/root/linux-2.6.34'
  CC [M]  /root/kprobe/mod1.o
  Building modules, stage 2.
  MODPOST 1 modules
  CC      /root/kprobe/mod1.mod.o
  LD [M]  /root/kprobe/mod1.ko
make[1]: Leaving directory '/root/linux-2.6.34'
#+end_example

When done, you are ready to test your example module by inserting it into the kernel:
# insmod mod1.ko

Confirm that the module is successfully inserted:
# lsmod | head -n 5
Module                  Size  Used by
mod1                     904  0
fuse                   46627  2
sunrpc                158985  1
xt_physdev              1355  1

Now, since you have used ip_rcv() as your target function, you need to invoke it with a simple ping:
# ping localhost

Run dmesg and find your module’s messages:
module inserted
Pre_Handler: counter=0
Post_Handler: counter=1
Pre_Handler: counter=2
Post_Handler: counter=3

As you see, you can probe a kernel address and do instrumentation without recompiling the kernel, as was required by the simple printk. When you are done with your debugging, don’t forget to remove the module:
# rmmod mod1

In the exit function, myexit(), Kprobe is unregistered by calling unregister_kprobe(&kp);.

However, Kprobe has limits to what you can do with it. In the above example, you have just printed some messages in the handlers; you cannot access the function’s arguments with Kprobe. Let’s move on to something better.
Probing with Jprobe

For those who like bonus features, Jprobe is another kind of probing technique, which can be used to access the target function’s arguments, and thus display what was passed to the function. The basics are the same as that of Kprobe, but this additional feature makes Jprobe an interesting tool.

To get the Jprobe structure details, look in the file include/linux/kprobes.h:
struct jprobe {
        struct kprobe kp;
        void *entry;    /* probe handling code to jump to */
};

As you see, it contains a struct kprobe member, plus a pointer to store the address of a handler function to jump to.
A Jprobe example
#+begin_src c	
#include<linux/module.h>
#include<linux/version.h>
#include<linux/kernel.h>
#include<linux/init.h>
#include<linux/kprobes.h>
#include<net/ip.h>
#include <linux/kallsyms.h>
 
int my_handler (struct sk_buff *skb, struct net_device *dev, struct packet_type *pt, struct net_device *orig_dev){
 
    struct iphdr *my_iph;
    u32 S_ip,D_ip;
    my_iph = ip_hdr(skb);
    S_ip = my_iph->saddr;
    D_ip = my_iph->daddr;
    printk("Source IP: \n"NIPQUAD_FMT,NIPQUAD(S_ip));
     jprobe_return();
}
 
static struct jprobe my_probe;
 
int myinit(void)
{
    int ret;
    //my_probe.kp.addr = (kprobe_opcode_t *)0xc071c9a9;
    my_probe.kp.addr = (kprobe_opcode_t *) kallsyms_lookup_name("ip_rcv");
    if (!my_probe.kp.addr) {
       printk("Couldn't find %s to plant jprobe\n", "ip_rcv");
       return -1;
    }
    my_probe.entry = (kprobe_opcode_t *)my_handler;
    if ((ret = register_jprobe(&my_probe)) < 0) {
       printk("register_jprobe failed, returned %d\n", ret);
       return -1;
    }
    return 0;
}
 
void myexit(void)
{
    unregister_jprobe(&my_probe);
    printk("module removed\n ");
}
 
module_init(myinit);
module_exit(myexit);
 
/*Kernel module Comments*/
MODULE_AUTHOR("Manoj");
MODULE_DESCRIPTION("SIMPLE MODULE");
MODULE_LICENSE("GPL");
//MODULE_LICENSE("GPL v2");
#+end_src
Code walk-through

The example is simple to understand, but let me explain things a bit. Here, in the myinit() function, you assigned the target function address to the addr member of the Kprobe member struct kp, just like for the earlier module. The main difference is that you’ve now assigned a single handler function, my_handler, to the entry member:
my_probe.entry = (kprobe_opcode_t *)my_handler;

You’ve probably already noted that the signature of the single handler function here is quite different from the Kprobe handlers. The reason is, the handler must have the same arguments as that of the kernel function you’re probing, which is once again ip_rcv():
int my_handler (struct sk_buff *skb, struct net_device *dev, struct packet_type *pt, struct net_device *orig_dev);
extern int   ip_rcv(struct sk_buff *skb, struct net_device *dev,  struct packet_type *pt, struct net_device *orig_dev);

Jprobe lets us access the arguments of a function by calling your handler with the same arguments passed to the target function. This means that when ip_rcv is called, its arguments can be accessed from your probe handler as it is able to refer to the function’s address space plus the components within that function stack.

The line my_iph = ip_hdr(skb); will extract the IP header from sk_buff. Then extract the source and destination IP addresses in dot notation form, using the NIPQUAD and NIPQUAD_FMT macros declared in include/linux/kernel.h, and print the addresses.

Now, compile your module, insert it, and check that the module has been inserted successfully, just as before. Again, to invoke ip_rcv(), run a ping and then run dmesg to check the output:
# ping www.google.com
# dmesg
Source IP: 192.168.1.1
Destination IP: 192.168.1.3
Source IP: 209.85.231.104
Destination IP: 192.168.1.3

The output shows that Jprobe lets you get the function’s argument values, which can be very handy when debugging data-dependent bugs.

修改一下
#+begin_src c	
#include<linux/module.h>
#include<linux/version.h>
#include<linux/kernel.h>
#include<linux/init.h>
#include<linux/kprobes.h>
static const char *probed_func = "ip_rcv"; 
static unsigned int counter = 0;
int Pre_Handler(struct kprobe *p, struct pt_regs *regs){
    printk("Pre_Handler: counter=%u\n",counter++);
    return 0;
}
 
void Post_Handler(struct kprobe *p, struct pt_regs *regs, unsigned long flags){
    printk("Post_Handler: counter=%u\n",counter++);
}
 
static struct kprobe kp;
 
int myinit(void)
{
    printk("module inserted\n ");
    kp.pre_handler = Pre_Handler;
    kp.post_handler = Post_Handler;
    /* 直接使用函数名 */
    kp.symbol_name = (char *)probed_func;
    register_kprobe(&kp);
    return 0;
}
 
void myexit(void)
{
    unregister_kprobe(&kp);
    printk("module removed\n ");
}
 
module_init(myinit);
module_exit(myexit);
MODULE_AUTHOR("Manoj");
MODULE_DESCRIPTION("KPROBE MODULE");
MODULE_LICENSE("GPL");
#+end_src

更详细的内容可以阅读linux-2.6.21/Documentation/kprobes.txt。
