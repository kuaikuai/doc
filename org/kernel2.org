* http://lxr.free-electrons.com/ident
* ip_options_build
#+BEGIN_SRC c 
void ip_options_build(struct sk_buff * skb, struct ip_options * opt,
			    u32 daddr, struct rtable *rt, int is_frag) 
{
	unsigned char * iph = skb->nh.raw;

	memcpy(&(IPCB(skb)->opt), opt, sizeof(struct ip_options));
	memcpy(iph+sizeof(struct iphdr), opt->__data, opt->optlen);
	opt = &(IPCB(skb)->opt);
	opt->is_data = 0;

	if (opt->srr)
		memcpy(iph+opt->srr+iph[opt->srr+1]-4, &daddr, 4);

	if (!is_frag) {
		if (opt->rr_needaddr)
			ip_rt_get_source(iph+opt->rr+iph[opt->rr+2]-5, rt);
		if (opt->ts_needaddr)
			ip_rt_get_source(iph+opt->ts+iph[opt->ts+2]-9, rt);
		if (opt->ts_needtime) {
			struct timeval tv;
			__u32 midtime;
			do_gettimeofday(&tv);
			midtime = htonl((tv.tv_sec % 86400) * 1000 + tv.tv_usec / 1000);
			memcpy(iph+opt->ts+iph[opt->ts+2]-5, &midtime, 4);
		}
		return;
	}
	if (opt->rr) {
		memset(iph+opt->rr, IPOPT_NOP, iph[opt->rr+1]);
		opt->rr = 0;
		opt->rr_needaddr = 0;
	}
	if (opt->ts) {
		memset(iph+opt->ts, IPOPT_NOP, iph[opt->ts+1]);
		opt->ts = 0;
		opt->ts_needaddr = opt->ts_needtime = 0;
	}
}

#+END_SRC
memcpy(iph+opt->srr+iph[opt->srr+1]-4, &daddr, 4);
opt->srr是srr在ip头中的偏移值
iph+opt->srr是指向了srr，
iph[opt->srr+1]获取了srr的长度
所以
iph+opt->srr+iph[opt->srr+1]-4 是指向srr的选项列表的最后一个项

memcpy(iph+opt->ts+iph[opt->ts+2]-5, &midtime, 4);
iph[opt->ts+2]获取ts指针的值，该指针实际上是1开始的偏移值，所以需要 -1 -4 = -5
* netdev_max_backlog作用

Receive packet steering简称rps，是google贡献给linux kernel的一个patch，主要的功能是解决多核情况下，网络协议栈的软中断的负载均衡。这里的负载均衡也就是指能够将软中断均衡的放在不同的cpu核心上运行。

简介在这里：
http://lwn.net/Articles/362339/

linux现在网卡的驱动支持两种模式，一种是NAPI，一种是非NAPI模式，这两种模式的区别，我前面的blog都有介绍，这里就再次简要的介绍下。

在NAPI中，中断收到数据包后调用__napi_schedule调度软中断，然后软中断处理函数中会调用注册的poll回掉函数中调用netif_receive_skb将数据包发送到3层，没有进行任何的软中断负载均衡。

在非NAPI中，中断收到数据包后调用netif_rx，这个函数会将数据包保存到input_pkt_queue，然后调度软中断，这里为了兼容NAPI的驱动，他的poll方法默认是process_backlog，最终这个函数会从input_pkt_queue中取得数据包然后发送到3层。

通过比较我们可以看到，不管是NAPI还是非NAPI的话都无法做到软中断的负载均衡，因为软中断此时都是运行在在硬件中断相应的cpu上。也就是说如果始终是cpu0相应网卡的硬件中断，那么始终都是cpu0在处理软中断，而此时cpu1就被浪费了，因为无法并行的执行多个软中断。

google的这个patch的基本原理是这样的,根据数据包的源地址，目的地址以及目的和源端口(这里它是将两个端口组合成一个4字节的无符数进行计算的，后面会看到)计算出一个hash值，然后根据这个hash值来选择软中断运行的cpu，从上层来看，也就是说将每个连接和cpu绑定，并通过这个hash值，来均衡软中断在多个cpu上。

这个介绍比较简单，我们来看代码是如何实现的。

它这里主要是hook了两个内核的函数，一个是netif_rx主要是针对非NAPI的驱动，一个是netif_receive_skb这个主要是针对NAPI的驱动。

在看netif_rx和netif_receive_skb之前，我们先来看这个patch中两个重要的函数get_rps_cpu和enqueue_to_backlog，我们一个个看。

先来看相关的两个数据结构，首先是netdev_rx_queue，它表示对应的接收队列，因为有的网卡可能硬件上就支持多队列的模式，此时对应就会有多个rx队列，这个结构是挂载在net_device中的，也就是每个网络设备最终都会有一个或者多个rx队列。这个结构在sys文件系统中的表示类似这样的/sys/class/net/<device>/queues/rx-<n> 几个队列就是rx-n.

#+begin_src c
    struct netdev_rx_queue {  
    //保存了当前队列的rps map  
        struct rps_map *rps_map;  
    //对应的kobject  
        struct kobject kobj;  
    //指向第一个rx队列  
        struct netdev_rx_queue *first;  
    //引用计数  
        atomic_t count;  
    } ____cacheline_aligned_in_smp;  
#+end_src


然后就是rps_map，其实这个也就是保存了能够执行数据包的cpu。
#+begin_src c
    struct rps_map {  
    //cpu的个数，也就是cpus数组的个数  
        unsigned int len;  
    //RCU锁  
        struct rcu_head rcu;  
    //保存了cpu的id.  
        u16 cpus[0];  
    };  
#+end_src


看完上面的结构，我们来看函数的实现。
get_rps_cpu主要是通过传递进来的skb然后来选择这个skb所应该被处理的cpu。它的逻辑很简单，就是通过skb计算hash，然后通过hash从对应的队列的rps_mapping中取得对应的cpu id。

这里有个要注意的就是这个hash值是可以交给硬件网卡去计算的，作者自己说是最好交由硬件去计算这个hash值，因为如果是软件计算的话会导致CPU 缓存不命中，带来一定的性能开销。

还有就是rps_mapping这个值是可以通过sys 文件系统设置的，位置在这里：
/sys/class/net/<device>/queues/rx-<n>/rps_cpus 。

#+begin_src c
    static int get_rps_cpu(struct net_device *dev, struct sk_buff *skb)  
    {  
        struct ipv6hdr *ip6;  
        struct iphdr *ip;  
        struct netdev_rx_queue *rxqueue;  
        struct rps_map *map;  
        int cpu = -1;  
        u8 ip_proto;  
        u32 addr1, addr2, ports, ihl;  
    //rcu锁  
        rcu_read_lock();  
    //取得设备对应的rx 队列  
        if (skb_rx_queue_recorded(skb)) {  
        ..........................................  
            rxqueue = dev->_rx + index;  
        } else  
            rxqueue = dev->_rx;  
      
        if (!rxqueue->rps_map)  
            goto done;  
    //如果硬件已经计算，则跳过计算过程  
        if (skb->rxhash)  
            goto got_hash; /* Skip hash computation on packet header */  
      
        switch (skb->protocol) {  
        case __constant_htons(ETH_P_IP):  
            if (!pskb_may_pull(skb, sizeof(*ip)))  
                goto done;  
    //得到计算hash的几个值  
            ip = (struct iphdr *) skb->data;  
            ip_proto = ip->protocol;  
    //两个地址  
            addr1 = ip->saddr;  
            addr2 = ip->daddr;  
    //得到ip头  
            ihl = ip->ihl;  
            break;  
        case __constant_htons(ETH_P_IPV6):  
    ..........................................  
            break;  
        default:  
            goto done;  
        }  
        ports = 0;  
        switch (ip_proto) {  
        case IPPROTO_TCP:  
        case IPPROTO_UDP:  
        case IPPROTO_DCCP:  
        case IPPROTO_ESP:  
        case IPPROTO_AH:  
        case IPPROTO_SCTP:  
        case IPPROTO_UDPLITE:  
            if (pskb_may_pull(skb, (ihl * 4) + 4))  
    //我们知道tcp头的前4个字节就是源和目的端口，因此这里跳过ip头得到tcp头的前4个字节  
                ports = *((u32 *) (skb->data + (ihl * 4)));  
            break;  
      
        default:  
            break;  
        }  
    //计算hash  
        skb->rxhash = jhash_3words(addr1, addr2, ports, hashrnd);  
        if (!skb->rxhash)  
            skb->rxhash = 1;  
      
    got_hash:  
    //通过rcu得到对应rps map  
        map = rcu_dereference(rxqueue->rps_map);  
        if (map) {  
    //取得对应的cpu  
            u16 tcpu = map->cpus[((u64) skb->rxhash * map->len) >> 32];  
    //如果cpu是online的，则返回计算出的这个cpu，否则跳出循环。  
            if (cpu_online(tcpu)) {  
                cpu = tcpu;  
                goto done;  
            }  
        }  
      
    done:  
        rcu_read_unlock();  
    //如果上面失败，则返回-1.  
        return cpu;  
    }  
#+end_src


然后是enqueue_to_backlog这个方法，首先我们知道在每个cpu都有一个softnet结构，而他有一个input_pkt_queue的队列，以前这个主要是用于非NAPi的驱动的，而这个patch则将这个队列也用与NAPI的处理中了。也就是每个cpu现在都会有一个input_pkt_queue队列，用于保存需要处理的数据包队列。这个队列作用现在是，如果发现不属于当前cpu处理的数据包，则我们可以直接将数据包挂载到他所属的cpu的input_pkt_queue中。

enqueue_to_backlog接受一个skb和cpu为参数，通过cpu来判断skb如何处理。要么加入所属的input_pkt_queue中，要么schecule 软中断。

还有个要注意就是我们知道NAPI为了兼容非NAPI模式，有个backlog的napi_struct结构，也就是非NAPI驱动会schedule backlog这个napi结构，而在enqueue_to_backlog中则是利用了这个结构，也就是它会schedule backlog，因为它会将数据放到input_pkt_queue中，而backlog的pool方法process_backlog就是从input_pkt_queue中取得数据然后交给上层处理。

这里还有一个会用到结构就是 rps_remote_softirq_cpus，它主要是保存了当前cpu上需要去另外的cpu schedule 软中断的cpu 掩码。因为我们可能将要处理的数据包放到了另外的cpu的input queue上，因此我们需要schedule 另外的cpu上的napi(也就是软中断),所以我们需要保存对应的cpu掩码，以便于后面遍历，然后schedule。

而这里为什么mask有两个元素，注释写的很清楚：
#+begin_src c
    /* 
     * This structure holds the per-CPU mask of CPUs for which IPIs are scheduled 
     * to be sent to kick remote softirq processing.  There are two masks since 
     * the sending of IPIs must be done with interrupts enabled.  The select field 
     * indicates the current mask that enqueue_backlog uses to schedule IPIs. 
     * select is flipped before net_rps_action is called while still under lock, 
     * net_rps_action then uses the non-selected mask to send the IPIs and clears 
     * it without conflicting with enqueue_backlog operation. 
     */  
    struct rps_remote_softirq_cpus {  
    //对应的cpu掩码  
        cpumask_t mask[2];  
    //表示应该使用的数组索引  
        int select;  
    };  
#+end_src

#+begin_src c
    static int enqueue_to_backlog(struct sk_buff *skb, int cpu)  
    {  
        struct softnet_data *queue;  
        unsigned long flags;  
    //取出传递进来的cpu的softnet-data结构  
        queue = &per_cpu(softnet_data, cpu);  
      
        local_irq_save(flags);  
        __get_cpu_var(netdev_rx_stat).total++;  
    //自旋锁  
        spin_lock(&queue->input_pkt_queue.lock);  
    //如果保存的队列还没到上限  
        if (queue->input_pkt_queue.qlen <= netdev_max_backlog) {  
    //如果当前队列的输入队列长度不为空  
            if (queue->input_pkt_queue.qlen) {  
    enqueue:  
    //将数据包加入到input_pkt_queue中,这里会有一个小问题，我们后面再说。  
                __skb_queue_tail(&queue->input_pkt_queue, skb);  
                spin_unlock_irqrestore(&queue->input_pkt_queue.lock,  
                    flags);  
                return NET_RX_SUCCESS;  
            }  
      
            /* Schedule NAPI for backlog device */  
    //如果可以调度软中断  
            if (napi_schedule_prep(&queue->backlog)) {  
    //首先判断数据包该不该当前的cpu处理  
                if (cpu != smp_processor_id()) {  
    //如果不该，  
                    struct rps_remote_softirq_cpus *rcpus =  
                        &__get_cpu_var(rps_remote_softirq_cpus);  
      
                    cpu_set(cpu, rcpus->mask[rcpus->select]);  
                    __raise_softirq_irqoff(NET_RX_SOFTIRQ);  
                } else  
    //如果就是应该当前cpu处理，则直接schedule 软中断，这里可以看到传递进去的是backlog  
                    __napi_schedule(&queue->backlog);  
            }  
            goto enqueue;  
        }  
      
        spin_unlock(&queue->input_pkt_queue.lock);  
      
        __get_cpu_var(netdev_rx_stat).dropped++;  
        local_irq_restore(flags);  
      
        kfree_skb(skb);  
        return NET_RX_DROP;  
    }  
#+end_src

这里会有一个小问题，那就是假设此时一个属于cpu0的包进入处理，此时我们运行在cpu1,此时将数据包加入到input队列，然后cpu0上面刚好又来了一个cpu0需要处理的数据包，此时由于qlen不为0则又将数据包加入到input队列中，我们会发现cpu0上的napi没机会进行调度了。

google的patch对这个是这样处理的，在软中断处理函数中当数据包处理完毕，会调用net_rps_action来调度前面保存到其他cpu上的input队列。

下面就是代码片断（net_rx_action）

#+begin_src c
    //得到对应的rcpus.  
    rcpus = &__get_cpu_var(rps_remote_softirq_cpus);  
        select = rcpus->select;  
    //翻转select，防止和enqueue_backlog冲突  
        rcpus->select ^= 1;  
      
    //打开中断，此时下面的调度才会起作用.  
        local_irq_enable();  
    //这个函数里面调度对应的远程cpu的napi.  
        net_rps_action(&rcpus->mask[select]);  
#+end_src


然后就是net_rps_action，这个函数很简单，就是遍历所需要处理的cpu，然后调度napi
#+begin_src c
    static void net_rps_action(cpumask_t *mask)  
    {  
        int cpu;  
      
        /* Send pending IPI's to kick RPS processing on remote cpus. */  
    //遍历  
        for_each_cpu_mask_nr(cpu, *mask) {  
            struct softnet_data *queue = &per_cpu(softnet_data, cpu);  
            if (cpu_online(cpu))  
    //到对应的cpu调用csd方法。  
                __smp_call_function_single(cpu, &queue->csd, 0);  
        }  
    //清理mask  
        cpus_clear(*mask);  
    }  
#+end_src

上面我们看到会调用csd方法，而上面的csd回掉就是被初始化为trigger_softirq函数。
#+begin_src c
    static void trigger_softirq(void *data)  
    {  
        struct softnet_data *queue = data;  
    //调度napi可以看到依旧是backlog 这个napi结构体。  
        __napi_schedule(&queue->backlog);  
        __get_cpu_var(netdev_rx_stat).received_rps++;  
    }  
#+end_src

上面的函数都分析完毕了，剩下的就很简单了。

首先来看netif_rx如何被修改的，它被修改的很简单，首先是得到当前skb所应该被处理的cpu id，然后再通过比较这个cpu和当前正在处理的cpu id进行比较来做不同的处理。

#+begin_src c
    int netif_rx(struct sk_buff *skb)  
    {  
        int cpu;  
      
        /* if netpoll wants it, pretend we never saw it */  
        if (netpoll_rx(skb))  
            return NET_RX_DROP;  
      
        if (!skb->tstamp.tv64)  
            net_timestamp(skb);  
    //得到cpu id。  
        cpu = get_rps_cpu(skb->dev, skb);  
        if (cpu < 0)  
            cpu = smp_processor_id();  
    //通过cpu进行队列不同的处理  
        return enqueue_to_backlog(skb, cpu);  
    }  
#+end_src

然后是netif_receive_skb,这里patch将内核本身的这个函数改写为__netif_receive_skb。然后当返回值小于0,则说明不需要对队列进行处理，此时直接发送到3层。
#+begin_src c
    int netif_receive_skb(struct sk_buff *skb)  
    {  
        int cpu;  
      
        cpu = get_rps_cpu(skb->dev, skb);  
      
        if (cpu < 0)  
            return __netif_receive_skb(skb);  
        else  
            return enqueue_to_backlog(skb, cpu);  
    }  
#+end_src


最后来总结一下，可以看到input_pkt_queue是一个FIFO的队列，而且如果当qlen有值的时候，也就是在另外的cpu有数据包放到input_pkt_queue中，则当前cpu不会调度napi，而是将数据包放到input_pkt_queue中，然后等待trigger_softirq来调度napi。

因此这个patch完美的解决了软中断在多核下的均衡问题，并且没有由于是同一个连接会map到相同的cpu，并且input_pkt_queue的使用，因此乱序的问题也不会出现。
  
* tcp_timestamp引发的问题
  tcp_tw_recycle 见[[tcp_tw_recycle]]
    近来线上陆续出现了一些connect失败的问题，经过分析试验，最终确认和proc参数tcp_tw_recycle/tcp_timestamps相关；
1. 现象
    第一个现象：模块A通过NAT网关访问服务S成功，而模块B通过NAT网关访问服务S经常性出现connect失败，抓包发现：服务S端已经收到了syn包，但没有回复synack；另外，模块A关闭了tcp timestamp，而模块B开启了tcp timestamp；
    第二个现象：不同主机上的模块C（开启timestamp），通过NAT网关（1个出口ip）访问同一服务S，主机C1 connect成功，而主机C2 connect失败；

2. 分析
    根据现象上述问题明显和tcp timestmap有关；查看linux 2.6.32内核源码，发现tcp_tw_recycle/tcp_timestamps都开启的条件下，60s内同一源ip主机的socket connect请求中的timestamp必须是递增的。
    源码函数：tcp_v4_conn_request(),该函数是tcp层三次握手syn包的处理函数（服务端）；
    源码片段：
   #+begin_src c
		/* VJ's idea. We save last timestamp seen
		 * from the destination in peer table, when entering
		 * state TIME-WAIT, and check against it before
		 * accepting new connection request.
		 *
		 * If "isn" is not zero, this request hit alive
		 * timewait bucket, so that all the necessary checks
		 * are made in the function processing timewait state.
		 */
		if (tmp_opt.saw_tstamp &&
		    tcp_death_row.sysctl_tw_recycle &&
		    (dst = inet_csk_route_req(sk, req)) != NULL &&
		    (peer = rt_get_peer((struct rtable *)dst)) != NULL &&
		    peer->v4daddr == saddr) {
			if (xtime.tv_sec < peer->tcp_ts_stamp + TCP_PAWS_MSL &&
			    (s32)(peer->tcp_ts - req->ts_recent) >
							TCP_PAWS_WINDOW) {
				NET_INC_STATS_BH(LINUX_MIB_PAWSPASSIVEREJECTED);
				dst_release(dst);
				goto drop_and_free;
			}
		}
   #+end_src
        tmp_opt.saw_tstamp：该socket支持tcp_timestamp
        sysctl_tw_recycle：本机系统开启tcp_tw_recycle选项
        TCP_PAWS_MSL：60s，该条件判断表示该源ip的上次tcp通讯发生在60s内
        TCP_PAWS_WINDOW：1，该条件判断表示该源ip的上次tcp通讯的timestamp 大于 本次tcp

    分析：主机client1和client2通过NAT网关（1个ip地址）访问serverN，由于timestamp时间为系统启动到当前的时间，因此，client1和client2的timestamp不相同；根据上述syn包处理源码，在tcp_tw_recycle和tcp_timestamps同时开启的条件下，timestamp大的主机访问serverN成功，而timestmap小的主机访问失败；

    参数：/proc/sys/net/ipv4/tcp_timestamps - 控制timestamp选项开启/关闭
          /proc/sys/net/ipv4/tcp_tw_recycle - 减少timewait socket释放的超时时间

    如果客户端是NAT出来的，并且我们server端有打开tcp_tw_recycle ,并且time stamp也没有关闭，那么假设第一个连接进来，然后关闭，此时这个句柄处于time wait状态，然后很快(小于60秒)又一个客户端(相同的源地址，如果打开了xfrm还要相同的端口号)发一个syn包，此时linux内核就会认为这个数据包异常的，因此就会丢掉这个包,并发送rst。

而现在大部分的客户端都是NAT出来的，因此建议tw_recycle还是关闭,或者说server段关闭掉time stamp(/proc/sys/net/ipv4/tcp_timestamps).

3. 解决方法
    echo 0 > /proc/sys/net/ipv4/tcp_tw_recycle;
    tcp_tw_recycle默认是关闭的，有不少服务器，为了提高性能，开启了该选项；
    为了解决上述问题，个人建议关闭tcp_tw_recycle选项，而不是timestamp；因为 在tcp timestamp关闭的条件下，开启tcp_tw_recycle是不起作用的；而tcp timestamp可以独立开启并起作用。
    源码函数：  tcp_time_wait()
    源码片段：
   #+begin_src c
        if (tcp_death_row.sysctl_tw_recycle && tp->rx_opt.ts_recent_stamp)
            recycle_ok = icsk->icsk_af_ops->remember_stamp(sk);
        ......
       
        if (timeo < rto)
            timeo = rto;

        if (recycle_ok) {
            tw->tw_timeout = rto;
        } else {
            tw->tw_timeout = TCP_TIMEWAIT_LEN;
            if (state == TCP_TIME_WAIT)
                timeo = TCP_TIMEWAIT_LEN;
        }

        inet_twsk_schedule(tw, &tcp_death_row, timeo,
                   TCP_TIMEWAIT_LEN);
   #+end_src
    timestamp和tw_recycle同时开启的条件下，timewait状态socket释放的超时时间和rto相关；否则，超时时间为TCP_TIMEWAIT_LEN，即60s；

    内核说明文档 对该参数的介绍如下：
    tcp_tw_recycle - BOOLEAN
    Enable fast recycling TIME-WAIT sockets. Default value is 0.
    It should not be changed without advice/request of technical
    experts.
* tcp_tw_reuse
  复用处于TIMEWAIT状态的sock
  inet_hash_connect()->__inet_check_established()->tcp_twsk_unique()
#+begin_src c
int tcp_twsk_unique(struct sock *sk, struct sock *sktw, void *twp)
{
	const struct tcp_timewait_sock *tcptw = tcp_twsk(sktw);
	struct tcp_sock *tp = tcp_sk(sk);

	/* With PAWS, it is safe from the viewpoint
	   of data integrity. Even without PAWS it is safe provided sequence
	   spaces do not overlap i.e. at data rates <= 80Mbit/sec.

	   Actually, the idea is close to VJ's one, only timestamp cache is
	   held not per host, but per port pair and TW bucket is used as state
	   holder.

	   If TW bucket has been already destroyed we fall back to VJ's scheme
	   and use initial timestamp retrieved from peer table.
	 */
	if (tcptw->tw_ts_recent_stamp &&
	    (twp == NULL || (sysctl_tcp_tw_reuse &&
			     xtime.tv_sec - tcptw->tw_ts_recent_stamp > 1))) {
		tp->write_seq = tcptw->tw_snd_nxt + 65535 + 2;
		if (tp->write_seq == 0)
			tp->write_seq = 1;
		tp->rx_opt.ts_recent	   = tcptw->tw_ts_recent;
		tp->rx_opt.ts_recent_stamp = tcptw->tw_ts_recent_stamp;
		sock_hold(sktw);
		return 1;
	}

	return 0;
}
#+end_src
* tcp_tw_recycle
  快速回收处于TIMEWAIT状态的sock
  见inet_twsk_schedule()
* TCPIP函数调用大致流程

**  ip_send_reply 
    ip_route_output_key 
    ip_push_pending_frames 


** tcp_v4_send_ack(发送ACK) 
    ip_send_reply 
        ip_route_output_key 
        ip_push_pending_frames 


** 用户子上而下的读函数都间接的调用了tcp_recvmsg 
tcp_recvmsg
skb_copy_datagram_iovec 
tcp_recv_urg(接受一个字节的URG数据) 

** UDP
UDP的写函数都调用了udp_sendmsg 
udp_sendmsg
    ip_route_output_flow 
    ip_append_data 
    udp_flush_pending_frames 
        ip_flush_pending_frames 
    udp_push_pending_frames 
        ip_push_pending_frames 

接收
硬件->IP层->运输层收到数据，添加到对应的SOCKET缓冲区中 
由ip_rcv间接调用 
udp_rcv 
    __udp4_lib_rcv 
        if(是多播或广播) 
            __udp4_lib_mcast_deliver 
                udp_queue_rcv_skb(对每个需要接受的UDP SOCKET缓冲调用) 
        __udp4_lib_lookup 
        udp_queue_rcv_skb 


把数据块sk_buff放到一个sock结构的接受缓存的末尾中 
udp_queue_rcv_skb 
    sock_queue_rcv_skb 
        skb_queue_tail 

用户子上而下的读函数都间接的调用了udp_recvmsg 
udp_recvmsg
__skb_recv_datagram 
skb_copy_datagram_iovec 
skb_copy_and_csum_datagram_iovec 

** 原始套接字 
RAW Socket的写函数都调用了raw_sendmsg 
raw_sendmsg
    ip_route_output_flow 
    if(设置了IP_HDRINCL选项，即自己构造ip头部) 
        raw_send_hdrinc
    else 
        ip_append_data 
        ip_flush_pending_frames或 
        ip_push_pending_frames 


自底向上的收包 
raw_rcv 
由ip_forward调用ip_call_ra_chain，然后再调用的raw_rcv 
raw_rcv 
sock_queue_rcv_skb 
skb_queue_tail 
sk->sk_data_ready = sock_def_readable 
waitqueue_active 
sk_wake_async 


用户子上而下的读函数都间接的调用了raw_recvmsg 
raw_recvmsg
skb_recv_datagram 
__skb_recv_datagram 
wait_for_packet(如果没有数据，则调用此函数等待数据) 


** ICMP 
在任何需要发送ICMP报文的时候都会调用此函数 
icmp_send 
    __ip_route_output_key 
        ip_route_output_slow 
    ip_route_output_key 
        ip_route_output_flow 
    icmp_push_reply    
        ip_append_data 
        ip_flush_pending_frames或 
        ip_push_pending_frames 


硬件->IP层->运输层收到ICMP数据，作出处理逻辑 
由ip_rcv间接调用 
icmp_rcv 
    完全就是icmp协议的处理逻辑，通过函数指针icmp_pointers[icmph->type].handler调用了一下函数中的某一个 
    icmp_discard 
    icmp_unreach 
    icmp_redirect 
    icmp_timestamp 
    icmp_address 
    icmp_address_reply 
    icmp_echo 


** 网络层 
IP发送 
网络层中主要的发送函数有以下三个：ip_push_pending_frames，ip_queue_xmit，raw_send_hdrinc 
ip_push_pending_frames 
将所有pending状态的IP分组组合成一个IP分组，并发送 
    ip_local_out 


ip_queue_xmit 
    ip_route_output_flow(找路由) 
    ip_local_out 


raw_send_hdrinc 
    NF_HOOK(dst_output) 


ip_local_out 
    __ip_local_out 
        nf_hook(dst_output) 
    dst_output 

路由选择 
ip_route_output_flow 
    __ip_route_output_key 
        ip_route_output_slow 

路由选择 
ip_route_output_slow 
    fib_lookup 
    ip_mkroute_output 
        __mkroute_output 
        rt_hash 
        rt_intern_hash 
            arp_bind_neighbour 
                __neigh_lookup_errno 
                    neigh_lookup 
                    neigh_create 


dst_output 
    dst->output = ip_output 
    NF_HOOK_COND(ip_finish_output) 
        dst_output 
        ip_fragment 
        ip_finish_output2 
            neigh_hh_output 
                hh->hh_output = dev_queue_xmit 
            dst->neighbour->output = neigh_resolve_output 
                neigh->ops->queue_xmit = dev_queue_xmit 

IP接受 
接收IPv4包，由netif_rx间接调用 
ip_rcv 
    NF_HOOK 
    ip_rcv_finish 
        ip_route_input 
        dst_input 
            dst->input(可能是ip_local_deliver或ip_forward) 
            if(是发给本地的包) 
                dst->input是ip_local_deliver 
                    NF_HOOK 
                    ip_local_deliver_finish 
                    ipprot->handler(可能是tcp_v4_rcv,udp_rcv,icmp_rcv,igmp_rcv) 
            else 
                dst->input是ip_forward 

更新路由 
ip_route_input 
    ip_route_input_mc(多播) 
        rt_hash 
        rt_intern_hash 
    ip_route_input_slow(其它) 
        ip_mkroute_input 
            __mkroute_input 
            rt_hash 
            rt_intern_hash 
每收到一个IP报文都会调用此函数更新路由表。ip_route_input函数的上半部分是在hash table寻找路由项，如果找到就返回。找不到才会调用后面的ip_route_input_mc或ip_route_input_slow来更新路由表。 


转发 
ip_forward 
ip_call_ra_chain 
raw_rcv 
    xfrm4_route_forward(处理路由) 
        xfrm_route_forward 
            __xfrm_route_forward 
                xfrm_lookup 
                    __xfrm_lookup 
                        xfrm_find_bundle 
                            afinfo->find_bundle = __xfrm4_find_bundle 
                        xfrm_bundle_create 
                            xfrm_dst_lookup 
                                afinfo->dst_lookup = xfrm4_dst_lookup 
                                    __ip_route_output_key 
                                        ip_route_output_slow 
    处理各个参数(在一定条件下发送ICMP) 
    ip_decrease_ttl(减少TTL) 
    NF_HOOK(ip_forward_finish) 
        dst_output 


** 链路层 
接收帧 
由硬件驱动在中断处理程序中直接调用netif_rx 
netif_rx 
    if(netpoll_rx函数与把数据拿走) 
        return 
    __skb_queue_tail(把所有收到的数据保存起来) 
    napi_schedule 
        __napi_schedule 
            __raise_softirq_irqoff(NET_RX_SOFTIRQ); 

在net_dev_init函数中初始化了软中断： 
open_softirq(NET_TX_SOFTIRQ, net_tx_action, NULL); 
open_softirq(NET_RX_SOFTIRQ, net_rx_action, NULL); 
所以NET_RX_SOFTIRQ中断的处理函数是net_rx_action，NET_TX_SOFTIRQ中断的处理函数是net_tx_action。需要让上层接收数据时，只要触发相应的软中断，如__raise_softirq_irqoff(NET_RX_SOFTIRQ)。内核就会在适当时机执行do_softirq来处理pending的软中断。 


net_rx_action 
    n->poll = process_backlog 
        netif_receive_skb 
            pt_prev->func = ip_rcv(在这里完成了交接) 
    __raise_softirq_irqoff(NET_RX_SOFTIRQ) 


发送帧 
dev_queue_xmit 
    rcu_read_lock_bh 
    if(设备有发送队列) 
        q->enqueue(将数据追加到发送队列，软中断处理函数net_tx_action会执行真正的发送工作) 
    else 
        dev_hard_start_xmit 
            dev->hard_start_xmit = el_start_xmit 
                调用outw汇编指令发送数据，够底层了 
    rcu_read_unlock_bh 


net_tx_action 
    __kfree_skb(释放已发送的，此时中断由dev_kfree_skb_irq函数发起) 
    qdisc_run 
        __qdisc_run 
qdisc_restart 
                dev_hard_start_xmit 
            netif_schedule 
    netif_schedule 


netif_schedule 
    __netif_schedule 
        raise_softirq_irqoff(NET_TX_SOFTIRQ)  


