#+OPTIONS: "\n:t"
taobao
http://blog.sina.com.cn/u/2015038597

http://www.linuxvirtualserver.org/zh/lvs3.html
* LVS tunnel 配置
1. 安装ipvsadmin

2. LB 配置
- 配置网络地址
#+begin_example
  #配置eth0的VIP地址
  /sbin/ifconfig eth0:0 192.168.1.100 broadcast 192.168.1.100 netmask 255.255.255.255 up
  #加路由
  /sbin/route add –host 192.168.1.100 dev eth0:0
#+end_example
- 配置网络系统参数（eth0为绑定VIP的网卡设备） TODO？下面四条是否有必要？
  echo "0" >/proc/sys/net/ipv4/ip_forward
  echo "1" >/proc/sys/net/ipv4/conf/all/send_redirects   
  echo "1" >/proc/sys/net/ipv4/conf/default/send_redirects
  echo "1" >/proc/sys/net/ipv4/conf/eth0/send_redirects
- 配置IPVS的服务类型、VIP地址以及对应的RS信息，例如：
#+begin_example
/sbin/ipvsadm -A -t 192.168.1.100:80 -s rr
/sbin/ipvsadm -a -t 192.168.1.100:80 -r 192.168.1.10 -i -w 1
/sbin/ipvsadm -a -t 192.168.1.100:80 -r 192.168.1.20 -i -w 1
#+end_example
3. RS 配置
- 配置网络地址
#+begin_example
  #配置tunl0
  /sbin/ifconfig tunl0 192.168.1.100 broadcast 192.168.1.100 netmask 255.255.255.255 up
  #增加路由
  /sbin/route add -host 192.168.1.100 dev tunl0
#+end_example
- 配置系统参数
  #关闭转发功能
  echo "0" >/proc/sys/net/ipv4/ip_forward
- 解决arp问题
#+begin_example
  echo 1 > /proc/sys/net/ipv4/conf/tunl0/arp_ignore
  echo 2 > /proc/sys/net/ipv4/conf/tunl0/arp_announce
  echo 1 > /proc/sys/net/ipv4/conf/all/arp_ignore
  echo 2 > /proc/sys/net/ipv4/conf/all/arp_announce
#+end_example
- 解决源地址验证问题
#+begin_example
  echo 0 > /proc/sys/net/ipv4/conf/tunl0/rp_filter
  echo 0 > /proc/sys/net/ipv4/conf/all/rp_filter
#+end_example
4. 在其他机器访问http://192.168.1.100:80测试

* ipvsmand
  #+CAPTION: ipvsmond 类图
  [[file:img/ipvsmon.png]]
1. ipvsmand
   监控lvs各种server
   ipvsmand
   流程：
   - 调用ipvsadm 加载/var/lib/ipvsadm目录下的配置文件
   - cfg解释/etc/ipvsman/topology.cfg文件到内存中
   - cfg解释/var/lib/ipvsadm下的配置文件到内存中
   - 获取topology.cfg的修改时间戳
   - 调用runRealDaemons启动监控线程，监控每个service的real
   - 启动查看topology.cfg修改的线程
     当topology.cfg修改时，该线程根据配置增减，调用ipvsadm增减service\real
   - 为信号SIGHUP设置处理函数
     当接受到SIGHUP时，重新执行上面的过程。
2. ipvsadm.py
   封装ipvsadm命令的各种功能

3. data.py
   包含 Service、Real两个类
   构造函数 Real(service, realArgs)
   其中service 为Service对象
   realArgs为该real的配置

4. plugins.py
   实现探测各个server可用的功能
   包含类Tester

5. config.py
   configparser.py
   包含 Tokenize、ParseToDict两个类
   ipvsmand的配置解释
   
   ParseToDict的主要函数为parseComplex

6. iptv_ipvsman 启动、停止ipvsmand服务的脚本

* LVS集群系统网络核心原理分析
** LVS结构与工作原理

　　LVS由前端的负载均衡器(Load Balancer，LB)和后端的真实服务器(Real Server，RS)群组成。RS间可通过局域网或广域网连接。
LVS的这种结构对用户是透明的，用户只能看见一台作为LB的虚拟服务器(Virtual Server)，而看不到提供服务的RS群。

　　当用户的请求发往虚拟服务器，LB根据设定的包转发策略和负载均衡调度算法将用户请求转发给RS。
    RS再将用户请求结果返回给用户。同请求包一样，应答包的返回方式也与包转发策略有关。

　　LVS的包转发策略有三种：
1. NAT (Network Address Translation)模式。
   LB收到用户请求包后，LB将请求包中虚拟服务器的IP地址转换为某个选定RS的IP地址，转发给RS；
   RS将应答包发给LB，LB将应答包中RS的IP转为虚拟服务器的IP地址，回送给用户。
2. IP隧道 (IP Tunneling)模式。
   LB收到用户请求包后，根据IP隧道协议封装该包，然后传给某个选定的RS；
   RS解出请求信息，直接将应答内容传给用户。此时要求RS和LB都要支持IP隧道协议。
3. DR(Direct Routing)模式。
   LB收到请求包后，将请求包中目标MAC地址转换为某个选定RS的MAC地址后将包转发出去，RS收到请求包后 ,可直接将应答内容传给用户。
   此时要求LB和所有RS都必须在一个物理段内,且LB与RS群共享一个虚拟IP。

   LVS软件的核心是运行在LB上的IPVS，它使用基于IP层的负载均衡方法。
   IPVS的总体结构主要由IP包处理、负载均衡算法、系统配置与管理三个模块及虚拟服务器与真实服务器链表组成。

** LVS对 IP包的处理模式

   IP包处理用Linux 2.4内核的Netfilter框架完成。一个数据包通过Netfilter框架的过程如图所示：
    
   通俗的说，netfilter的架构就是在整个网络流程的若干位置放置了一些检测点（HOOK），
   而在每个检测点上上登记了一些处理函数进行处理（如包过滤，NAT等，甚至可以是用户自定义的功能
   NF_IP_PRE_ROUTING：刚刚进入网络层的数据包通过此点（刚刚进行完版本号，校验和等检测），源地址转换在此点进行；
   NF_IP_LOCAL_IN：经路由查找后，送往本机的通过此检查点,INPUT包过滤在此点进行；
   NF_IP_FORWARD：要转发的包通过此检测点,FORWORD包过滤在此点进行；
   NF_IP_LOCAL_OUT：本机进程发出的包通过此检测点，OUTPUT包过滤在此点进行；
   NF_IP_POST_ROUTING：所有马上便要通过网络设备出去的包通过此检测点，内置的目的地址转换功能（包括地址伪装）在此点进行。

   在IP层代码中，有一些带有NF_HOOK宏的语句，如IP的转发函数中有：
   NF_HOOK(PF_INET, NF_IP_FORWARD, skb, skb->dev, dev2,ip_forward_finish);
   //其中NF_HOOK宏的定义基本如下：
#+begin_src c
   #ifdef CONFIG_NETFILTER
   #define NF_HOOK(pf, hook, skb, indev, outdev, okfn)
   (list_empty(&nf_hooks[(pf)][(hook)])
   ? (okfn)(skb)
   : nf_hook_slow((pf), (hook), (skb), (indev), (outdev), (okfn)))
   #else /* !CONFIG_NETFILTER */
   #define NF_HOOK(pf, hook, skb, indev, outdev, okfn) (okfn)(skb)
   #endif /*CONFIG_NETFILTER*/
#+end_src
   如果在编译内核时没有配置netfilter时，就相当于调用最后一个参数，此例中即执行ip_forward_finish函数；
   否则进入HOOK 点，执行通过nf_register_hook（）登记的功能（这句话表达的可能比较含糊，实际是进入nf_hook_slow（）函数，再由它执行登记的函数）。

   NF_HOOK宏的参数分别为：

   pf：协议族名，netfilter架构同样可以用于IP层之外，因此这个变量还可以有诸如PF_INET6，PF_DECnet等名字。
   hook：HOOK点的名字，对于IP层，就是取上面的五个值；
   skb：顾名思义
   indev：进来的设备，以struct net_device结构表示；
   outdev：出去的设备，以struct net_device结构表示；
   okfn:是个函数指针，当所有的该HOOK点的所有登记函数调用完后，转而走此流程。

   内核中定义好的，除非你是这部分内核代码的维护者，否则无权增加或修改，而在此检测点进行的处理，则可由用户指定。
   像packet filter,NAT,connection track这些功能，也是以这种方式提供的。正如netfilter的当初的设计目标－－提供一个完善灵活的框架，为扩展功能提供方便。

   如果我们想加入自己的代码,便要用nf_register_hook函数，其函数原型为：
#+begin_src c
   int nf_register_hook(struct nf_hook_ops *reg)
   struct nf_hook_ops：//结构
   struct nf_hook_ops
   {
   struct list_head list;
   /* User fills in from here down. */
   nf_hookfn *hook;
   int pf;
   int hooknum;
   /* Hooks are ordered in ascending priority. */
   int priority;
   };
#+end_src

   　　其实，类似LVS的做法就是生成一个struct nf_hook_ops结构的实例，并用nf_register_hook将其HOOK上。其中list项要初始化为{NULL,NULL}；
   由于一般在 IP层工作，pf总是PF_INET；hooknum就是HOOK点;一个HOOK点可能挂多个处理函数，谁先谁后，便要看优先级，即priority的指定了。
   netfilter_ipv4.h中用一个枚举类型指定了内置的处理函数的优先级：
#+begin_src c
enum nf_ip_hook_priorities {
NF_IP_PRI_FIRST = INT_MIN,
NF_IP_PRI_CONNTRACK = -200,
NF_IP_PRI_MANGLE = -150,
NF_IP_PRI_NAT_DST = -100,
NF_IP_PRI_FILTER = 0,
NF_IP_PRI_NAT_SRC = 100,
NF_IP_PRI_LAST = INT_MAX,
};
#+end_src
　　hook是提供的处理函数，也就是我们的主要工作，其原型为：
#+begin_src c
unsigned int nf_hookfn(unsigned int hooknum,
struct sk_buff **skb,
const struct net_device *in,
const struct net_device *out,
int (*okfn)(struct sk_buff *));
#+end_src
　　它的五个参数将由NFHOOK宏传进去。

　　以上是NetFillter编写自己模块时的一些基本用法，接下来，我们来看一下LVS中是如何实现的。

*** LVS中Netfiler的实现

    利用Netfilter，LVS处理数据报从左边进入系统，进行IP校验以后，数据报经过第一个钩子函数NF_IP_PRE_ROUTING [HOOK1]进行处理；
    然后进行路由选择，决定该数据报是需要转发还是发给本机；若该数据报是发被本机的，则该数据经过钩子函数 NF_IP_LOCAL_IN[HOOK2]处理后传递给上层协议；
    若该数据报应该被转发，则它被NF_IP_FORWARD[HOOK3]处理；经过转发的数据报经过最后一个钩子函数NF_IP_POST_ROUTING[HOOK4]处理以后，再传输到网络上。
    本地产生的数据经过钩子函数 NF_IP_LOCAL_OUT[HOOK5]处理后，进行路由选择处理，然后经过NF_IP_POST_ROUTING[HOOK4]处理后发送到网络上。

    当启动IPVS加载ip_vs模块时，模块的初始化函数ip_vs_init( )注册了NF_IP_LOCAL_IN[HOOK2]、NF_IP_FORWARD[HOOK3]、NF_IP_POST_ROUTING[HOOK4] 钩子函数用于处理进出的数据报。

**** NF_IP_LOCAL_IN处理过程

　　用户向虚拟服务器发起请求，数据报经过NF_IP_LOCAL_IN[HOOK2],进入ip_vs_in( )进行处理。
1. 如果传入的是icmp数据报，则调用ip_vs_in_icmp( )；
2. 否则继续判断是否为tcp/udp数据报，如果不是tcp/udp数据报，则函数返回NF_ACCEPT(让内核继续处理该数据报)；
3. 余下情况便是处理tcp/udp数据报。
   首先，调用ip_vs_header_check( )检查报头，如果异常，则函数返回NF_DROP(丢弃该数据报)。
   接着，调用ip_vs_conn_in_get( )去ip_vs_conn_tab表中查找是否存在这样的连接：它的客户机和虚拟服务器的ip地址和端口号以及协议类型均与数据报中的相应信息一致。
   如果不存在相应连接，则意味着连接尚未建立，
   此时如果数据报为tcp的sync报文或udp数据报则查找相应的虚拟服务器；如果相应虚拟服务器存在但是已经满负荷，则返回NF_DROP；
   如果相应虚拟服务器存在并且未满负荷，那么调用ip_vs_schedule( )调度一个RS并创建一个新的连接，如果调度失败则调用ip_vs_leave( )继续传递或者丢弃数据报。
   如果存在相应连接，首先判断连接上的RS是否可用，如果不可用则处理相关信息后返回NF_DROP。找到已存在的连接或建立新的连接后，修改系统记录的相关信息如传入的数据报的个数等。
   如果这个连接在创建时绑定了特定的数据报传输函数，调用这个函数传输数据报，否则返回 NF_ACCEPT。

　　ip_vs_in()调用的ip_vs_in_icmp( )处理icmp报文。函数开始时检查数据报的长度，如果异常则返回NF_DROP。
函数只处理由tcp/udp报文传送错误引起的目的不可达、源端被关闭或超时的icmp报文，其他情况则让内核处理。
针对上述三类报文，首先检查检验和。如果检验和错误，直接返回NF_DROP；否则，分析返回的icmp差错信息，查找相应的连接是否存在。
如果连接不存在，返回NF_ACCEPT；如果连接存在，根据连接信息，依次修改差错信息包头的ip地址与端口号及 ICMP数据报包头的ip地址，
并重新计算和修改各个包头中的检验和，之后查找路由调用ip_send( )发送修改过的数据报，并返回NF_STOLEN(退出数据报的处理过程)。

　　ip_vs_in()调用的函数ip_vs_schedule( )为虚拟服务器调度可用的RS并建立相应连接。它将根据虚拟服务器绑定的调度算法分配一个RS，
如果成功，则调用ip_vs_conn_new( )建立连接。ip_vs_conn_new( )将进行一系列初始化操作：设置连接的协议、ip地址、端口号、协议超时信息，
绑定application helper、RS和数据报传输函数，最后调用ip_vs_conn_hash( )将这个连接插入哈希表ip_vs_conn_tab中。
一个连接绑定的数据报传输函数，依据IPVS工作方式可分为ip_vs_nat_xmit( )、ip_vs_tunnel_xmit( )、ip_vs_dr_xmit( )。
例如ip_vs_nat_xmit( )的主要操作是：修改报文的目的地址和目的端口为RS信息，重新计算并设置检验和，调用ip_send( )发送修改后的数据报。

**** NF_IP_FORWARD处理过程

　　数据报进入NF_IP_FORWARD后，将进入ip_vs_out( )进行处理。这个函数只在NAT方式下被调用。它首先判断数据报类型，如果为icmp数据报则直接调用ip_vs_out_icmp( )；
其次判断是否为tcp/udp数据报，如果不是这二者则返回NF_ACCEPT。
余下就是tcp/udp数据报的处理。首先，调用 ip_vs_header_check( )检查报头，如果异常则返回NF_DROP。
其次，调用ip_vs_conn_out_get( )判断是否存在相应的连接。
1. 若不存在相应连接
   调用ip_vs_lookup_real_service( )去哈希表中查找发送数据报的RS是否仍然存在，
   如果RS存在且报文是tcp非复位报文或udp 报文，则调用icmp_send( )给RS发送目的不可达icmp报文并返回NF_STOLEN；
   其余情况下均返回NF_ACCEPT。
2. 若存在相应连接
   检查数据报的检验和，如果错误则返回NF_DROP，
   如果正确，修改数据报，将源地址修改为虚拟服务器ip地址，源端口修改为虚拟服务器端口号，重新计算并设置检验和，并返回 NF_ACCEPT。

　　ip_vs_out_icmp( )的流程与ip_vs_in_icmp( )类似，只是修改数据报时有所区别：
   ip报头的源地址和差错信息中udp或tcp报头的目的地址均修改为虚拟服务器地址，差错信息中udp或tcp报头的目的端口号修改为虚拟服务器的端口号。

**** NF_IP_POST_ROUTING处理过程

　　NF_IP_POST_ROUTING钩子函数只在NAT方式下使用。数据报进入NF_IP_POST_ROUTING后,由 ip_vs_post_routing( )进行处理。
   它首先判断数据报是否经过IPVS，如果未经过则返回NF_ACCEPT；否则立刻传输数据报，函数返回NF_STOLEN，防止数据报被 iptable的规则修改。
** LVS系统配置与管理

   IPVS模块初始化时注册了setsockopt/getsockopt( )，ipvsadm命令调用这两个函数向IPVS内核模块传递ip_vs_rule_user结构的系统配置数据，完成系统的配置，实现虚拟服务器和RS 地址的添加、修改、删除操作。系统通过这些操作完成对虚拟服务器和RS链表的管理。

   虚拟服务器的添加操作由ip_vs_add_service( )完成，该函数根据哈希算法向虚拟服务器哈希表添加一个新的节点，查找用户设定的调度算法并将此算法绑定到该节点；
   虚拟服务器的删除由ip_vs_del_service拟服务器的修改由 ip_vs_edit_service( )完成，此函数修改指定服务器的调度算法；
   虚拟服务器的删除由ip_vs_del_service( )完成，在删除一个虚拟服务器之前，必须先删除此虚拟服务器所带的所有RS，并解除虚拟服务器所绑定的调度算法。

   与之类似，RS的添加、修改、删除操作分别由ip_vs_add_dest( )、ip_vs_edit_dest( )和ip_vs_edit_dest( )完成。

** 负载均衡调度算法

　　前面已经提到，用户在添加一个虚拟服务时要绑定调度算法，这由ip_vs_bind_scheduler( )完成，调度算法的查找则由ip_vs_scheduler_get( )完成。
ip_vs_scheduler_get( )根据调度算法的名字，调用ip_vs_sched_getbyname( )从调度算法队列中查找此调度算法，如果没找到则加载相应调度算法模块再查找，最后返回查找结果。

目前系统有八种负载均衡调度算法，具体如下:

rr：轮循调度(Round-Robin) 它将请求依次分配不同的RS，也就是在RS中均摊请求。这种算法简单，但是只适合于RS处理性能相差不大的情况。
wrr：加权轮循调度(Weighted Round-Robin) 它将依据不同RS的权值分配任务。权值较高的RS将优先获得任务，并且分配到的连接数将比权值较低的RS更多。相同权值的RS得到相同数目的连接数。
dh：目的地址哈希调度 (Destination Hashing) 以目的地址为关键字查找一个静态hash表来获得需要的RS。
sh：源地址哈希调度(Source Hashing) 以源地址为关键字查找一个静态hash表来获得需要的RS。
Lc：最小连接数调度(Least-Connection) IPVS表存储了所有的活动的连接。把新的连接请求发送到当前连接数最小的RS。
Wlc：加权最小连接数调度(Weighted Least-Connection) 假设各台RS的权值依次为Wi（I = 1..n），当前的TCP连接数依次为Ti（I＝1..n），依次选取Ti/Wi为最小的RS作为下一个分配的RS。
Lblc：基于地址的最小连接数调度(Locality-Based Least-Connection) 将来自同一目的地址的请求分配给同一台RS如果这台服务器尚未满负荷，否则分配给连接数最小的RS，并以它为下一次分配的首先考虑。
Lblcr：基于地址的带重复最小连接数调度(Locality-Based Least-Connection with Replication) 对于某一目的地址，对应有一个RS子集。对此地址的请求，为它分配子集中连接数最小的RS；如果子集中所有的服务器均已满负荷，则从集群中选择一个连接数较小的服务器，将它加入到此子集并分配连接；若一定时间内，这个子集未被做任何修改，则将子集中负载最大的节点从子集删除。

* LVS 其他
1. ipvs分为三种负载均衡模式

  NAT、tunnel、direct routing（DR）
  NAT：所有交互数据必须通过均衡器
  tunnel：半连接处理方式，进行了IP封装
  DR：修改MAC地址，需要同一网段。

2. ipvs支持的均衡调度算法

  轮叫调度（Round-Robin Scheduling） 
  加权轮叫调度（Weighted Round-Robin Scheduling） 
  最小连接调度（Least-Connection Scheduling） 
  加权最小连接调度（Weighted Least-Connection Scheduling） 
  基于局部性的最少链接（Locality-Based Least Connections Scheduling） 
  带复制的基于局部性最少链接（Locality-Based Least Connections with Replication Scheduling） 
  目标地址散列调度（Destination Hashing Scheduling） 
  源地址散列调度（Source Hashing Scheduling）

3. ipvs代码记录

  内核为 Linux-kernel 3.3.7
1)  结构体
    ipvs各结构体定义在include\net\ip_vs.h与include\linux\ip_vs.h头文件中
    - struct ip_vs_protocol

      这个结构用来描述ipvs支持的IP协议。ipvs的IP层协议支持TCP, UDP, AH和ESP这4种IP层协议
    - struct ip_vs_conn
      这个结构用来描述ipvs的链接
    - struct ip_vs_service
      这个结构用来描述ipvs对外的虚拟服务器信息
    - struct ip_vs_dest
      这个结构用来描述具体的真实服务器信息
    - struct ip_vs_scheduler
      这个结构用来描述ipvs调度算法，目前调度方法包括rr，wrr，lc, wlc, lblc, lblcr, dh, sh等
    - struct ip_vs_app
      这个结构用来描述ipvs的应用模块对象
    - struct ip_vs_service_user
      这个结构用来描述ipvs用户空间的虚拟服务信息
    - struct ip_vs_dest_user
      这个结构用来描述ipvs用户空间的真实服务器信息
    - struct ip_vs_stats_user
      这个结构用来描述ipvs用户空间的统计信息
    - struct ip_vs_getinfo
      这个结构用来描述ipvs用户空间的获取信息
    - struct ip_vs_service_entry
      这个结构用来描述ipvs用户空间的服务规则项信息
    - struct ip_vs_dest_entry
      这个结构用来描述ipvs用户空间的真实服务器规则项信息
    - struct ip_vs_get_dests
      这个结构用来描述ipvs用户空间的获取真实服务器项信息
    - struct ip_vs_get_services
      这个结构用来描述ipvs用户空间的获取虚拟服务项信息
    - struct ip_vs_timeout_user
      这个结构用来描述ipvs用户空间的超时信息
    - struct ip_vs_daemon_user
      这个结构用来描述ipvs的内核守护进程信息

2) 模块初始化

  - ipvs服务初始化
    net\netfilter\ipvs\ip_vs_core.c文件
    static int __init ip_vs_init(void)

  - ioctl初始化
    net\netfilter\ipvs\ip_vs_ctl.c文件
    int __init ip_vs_control_init(void)

  - 协议初始化
    net\netfilter\ipvs\ip_vs_proto.c文件
    int __init ip_vs_protocol_init(void)

  - 连接初始化
    net\netfilter\ipvs\ip_vs_conn.c文件
    int __init ip_vs_conn_init(void)

  - netfilter挂接点数组，具体的数据包处理见数组中对应.hook的函数
    net\netfilter\ipvs\ip_vs_core.c文件
    static struct nf_hook_ops ip_vs_ops[]
    ret = nf_register_hooks(ip_vs_ops, ARRAY_SIZE(ip_vs_ops));



 

3) 调度算法具体实现

  各算法与ip_vs_scheduler结构体对应

  rr算法在net\netfilter\ipvs\ip_vs_rr.c文件中实现，以此类推。
#+begin_src c
static struct ip_vs_scheduler ip_vs_rr_scheduler = {
.name =                        "rr",                        /* name */
.refcnt =                ATOMIC_INIT(0),
.module =                THIS_MODULE,
.n_list =                LIST_HEAD_INIT(ip_vs_rr_scheduler.n_list),
.init_service =                ip_vs_rr_init_svc,
.update_service =        ip_vs_rr_update_svc,
.schedule =                ip_vs_rr_schedule,
};
#+end_src
- init_service
  算法初始化，在虚拟服务ip_vs_service和调度器绑定时调用(ip_vs_bind_scheduler()函数)
- update_service()
  函数在目的服务器变化时调用(如ip_vs_add_dest(), ip_vs_edit_dest()等函数)
  而算法核心函数schedule()则是在ip_vs_schedule()函数中在新建IPVS连接前调用，找到真正的服务器提供服务，建立IPVS连接。

4) 连接管理
   - struct ip_vs_conn *ip_vs_conn_in_get(const struct ip_vs_conn_param *p)
     进入方向
   - struct ip_vs_conn *ip_vs_conn_out_get(const struct ip_vs_conn_param *p)
     发出方向
   - struct ip_vs_conn * ip_vs_conn_new(...)
     建立连接
   - void ip_vs_bind_dest(struct ip_vs_conn *cp, struct ip_vs_dest *dest)
     绑定真实服务器
   - int ip_vs_bind_app(struct ip_vs_conn *cp, struct ip_vs_protocol *pp)
     绑定应用协议
   - static inline void ip_vs_bind_xmit(struct ip_vs_conn *cp)
     绑定发送方法
   - static inline int ip_vs_conn_hash(struct ip_vs_conn *cp)
     将连接结构添加到连接hash表
   - static inline int ip_vs_conn_unhash(struct ip_vs_conn *cp)
     从连接hash表中断开
   - static void ip_vs_conn_expire(unsigned long data)
     连接超时
   - static inline void ip_vs_control_del(struct ip_vs_conn *cp)
     从主连接中断开
   - void ip_vs_unbind_app(struct ip_vs_conn *cp)
     解除与应用的绑定
   - static inline void ip_vs_unbind_dest(struct ip_vs_conn *cp)
     接触与真实服务器的绑定
   - static void ip_vs_conn_flush(struct net *net)
     释放所有连接
   - void ip_vs_random_dropentry(struct net *net)
     定时随即删除连接
   - static inline int todrop_entry(struct ip_vs_conn *cp)
     判断是否要删除连接

3.5、协议管理

   - static int __used __init register_ip_vs_protocol(struct ip_vs_protocol *pp)
     注册一个ipvs协议
   - static int unregister_ip_vs_protocol(struct ip_vs_protocol *pp)
     注销一个ipvs协议
   - struct ip_vs_protocol * ip_vs_proto_get(unsigned short proto)
     查找服务,返回服务结构指针
   - void ip_vs_protocol_timeout_change(struct netns_ipvs *ipvs, int flags)
     修改协议超时标记
   - int *ip_vs_create_timeout_table(int *table, int size)
     创建状态超时表
   - int ip_vs_set_state_timeout(int *table, int num, const char *const *names, const char *name, int to)
     修改状态超时表
   - const char * ip_vs_state_name(__u16 proto, int state)
     返回协议状态名称
下面以TCP协议的实现来详细说明，相关代码文件为net\netfilter\ipvs\ip_vs_proto_tcp.c
#+begin_src c
struct ip_vs_protocol ip_vs_protocol_tcp = {
.name =                        "TCP",
.protocol =                IPPROTO_TCP,
.num_states =                IP_VS_TCP_S_LAST,
.dont_defrag =                0,
.init =                        NULL,
.exit =                        NULL,
.init_netns =                __ip_vs_tcp_init,
.exit_netns =                __ip_vs_tcp_exit,
.register_app =                tcp_register_app,
.unregister_app =        tcp_unregister_app,
.conn_schedule =        tcp_conn_schedule,
.conn_in_get =                ip_vs_conn_in_get_proto,
.conn_out_get =                ip_vs_conn_out_get_proto,
.snat_handler =                tcp_snat_handler,
.dnat_handler =                tcp_dnat_handler,
.csum_check =                tcp_csum_check,
.state_name =                tcp_state_name,
.state_transition =        tcp_state_transition,
.app_conn_bind =        tcp_app_conn_bind,
.debug_packet =                ip_vs_tcpudp_debug_packet,
.timeout_change =        tcp_timeout_change,
};
#+end_src
   - static void __ip_vs_tcp_init(struct net *net, struct ip_vs_proto_data *pd)
     tcp初始化函数
   - static void __ip_vs_tcp_exit(struct net *net, struct ip_vs_proto_data *pd)
     tcp退出函数
   - static int tcp_register_app(struct net *net, struct ip_vs_app *inc)
     注册tcp应用协议
   - static voidtcp_unregister_app(struct net *net, struct ip_vs_app *inc)
     注销tcp应用协议
   - static int tcp_conn_schedule(int af, struct sk_buff *skb, struct ip_vs_proto_data *pd, int *verdict, struct ip_vs_conn **cpp)
     tcp连接调度，该函数在ip_vs_in()函数中调用。
   - struct ip_vs_conn * ip_vs_conn_in_get_proto(int af, const struct sk_buff *skb, const struct ip_vs_iphdr *iph, unsigned int proto_off, int inverse)
     进入方向连接查找
   - struct ip_vs_conn * ip_vs_conn_out_get_proto(int af, const struct sk_buff *skb, const struct ip_vs_iphdr *iph, unsigned int proto_off, int inverse)
     发出方向连接查找
   - static int tcp_snat_handler(struct sk_buff *skb, struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
     该函数完成对协议部分数据进行源NAT操作,对TCP来说,NAT部分的数据就是源端口
   - static inline void tcp_fast_csum_update(int af, struct tcphdr *tcph, const union nf_inet_addr *oldip, const union nf_inet_addr *newip, __be16 oldport, __be16 newport)
     TCP校验和快速计算法,因为只修改了端口一个参数,可根据RFC1141方法快速计算
   - static int tcp_dnat_handler(struct sk_buff *skb, struct ip_vs_protocol *pp, struct ip_vs_conn *cp)
     该函数完成对协议部分数据进行目的NAT操作,对TCP来说,NAT部分的数据就是目的端口
   - static int tcp_csum_check(int af, struct sk_buff *skb, struct ip_vs_protocol *pp)
     计算IP协议中的校验和,对于TCP,UDP头中都有校验和参数,TCP中的校验和是必须的,而UDP的校验和可以不用计算。
     该函数用的都是linux内核提供标准的校验和计算函数

   - static const char * tcp_state_name(int state)
     该函数返回协议状态名称字符串
static const char *const tcp_state_name_table[IP_VS_TCP_S_LAST+1] = {
[IP_VS_TCP_S_NONE]                =        "NONE",
[IP_VS_TCP_S_ESTABLISHED]        =        "ESTABLISHED",
[IP_VS_TCP_S_SYN_SENT]                =        "SYN_SENT",
[IP_VS_TCP_S_SYN_RECV]                =        "SYN_RECV",
[IP_VS_TCP_S_FIN_WAIT]                =        "FIN_WAIT",
[IP_VS_TCP_S_TIME_WAIT]                =        "TIME_WAIT",
[IP_VS_TCP_S_CLOSE]                =        "CLOSE",
[IP_VS_TCP_S_CLOSE_WAIT]        =        "CLOSE_WAIT",
[IP_VS_TCP_S_LAST_ACK]                =        "LAST_ACK",
[IP_VS_TCP_S_LISTEN]                =        "LISTEN",
[IP_VS_TCP_S_SYNACK]                =        "SYNACK",
[IP_VS_TCP_S_LAST]                =        "BUG!",
};

TCP协议状态名称定义

static void tcp_state_transition(struct ip_vs_conn *cp, int direction, const struct sk_buff *skb, struct ip_vs_proto_data *pd)

tcp状态转换

static inline void set_tcp_state(struct ip_vs_proto_data *pd, struct ip_vs_conn *cp, int direction, struct tcphdr *th)

设置tcp连接状态

static struct tcp_states_t tcp_states []

tcp状态转换表

static void tcp_timeout_change(struct ip_vs_proto_data *pd, int flags)

超时变化

static int tcp_app_conn_bind(struct ip_vs_conn *cp)

本函数实现将多连接应用协议处理模块和IPVS连接进行绑定

* proc
/proc/sys/net/ipv4/vs/amemthresh
/proc/sys/net/ipv4/vs/am_droprate
/proc/sys/net/ipv4/vs/drop_entry
/proc/sys/net/ipv4/vs/drop_packet
/proc/sys/net/ipv4/vs/secure_tcp

/proc/sys/net/ipv4/vs/debug_level 


  代码见ip_vs_ctl.c
  /proc/net/ip_vs
  ip_vs_conn.c
  /proc/net/ip_vs_conn

在18上运行lvs director.
#+begin_example
bss-18:~ # ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.0.64.244:8080 rr
  -> 10.0.64.13:8080              Tunnel  1      0          0         
  -> 10.0.64.117:8080             Tunnel  1      0          1    
#+end_example

访问10.0.64.244:8080后
#+begin_example
bss-18:~ # cat /proc/net/ip_vs_conn
Pro FromIP   FPrt ToIP     TPrt DestIP   DPrt State       Expires PEName PEData
TCP 0A004013 9B2C 0A0040F4 1F90 0A004075 1F90 ESTABLISHED     897
#+end_example
再次查看
#+begin_example
bss-18:~ # cat /proc/net/ip_vs_conn
Pro FromIP   FPrt ToIP     TPrt DestIP   DPrt State       Expires PEName PEData
TCP 0A004013 9B2C 0A0040F4 1F90 0A004075 1F90 FIN_WAIT        110
#+end_example
* LVS tcp状态转换
  LVS根据tcp头中tcpflags，来维护简单的状态机。
  根据对应的状态，对每一个连接设置合适的超时时间。

  ip_vs_in()->ip_vs_set_state()->set_tcp_state()

  ip_vs_proto_tcp.c
  set_tcp_state():
  ...
  设置根据链接的状态，链接的超时时间
  cp->timeout = pp->timeout_table[cp->state = new_state];
* LVS Director RealServer 端口问题
  在使用Tunnel和Director模式时，
  通过ipvsadm 设置RealServer的端口异于Director的端口时，自动改成Director的端口.
  因为这两种模式不会修改4层的报文。

  有这需求时需要使用NAT模式
* 运行中常见问题
查看
ipvsadm -Ln
ipvsadm -Ln --stats
ipvsadm -Ln --rate

cat /proc/net/ip_vs
cat /proc/net/ip_vs_conn
cat /proc/net/ip_vs_conn_sync
** RealServer 上 rp_filter 被打开
在LVS上执行 cat /proc/net/ip_vs_conn
看是否有大量链接处于SYN的状态

** no destination available
   dmesg 查看发现有IPVS: no destination available之类的日志
   现场采用Round-Robin方式调度负载
#+begin_src c
/*
 * Round-Robin Scheduling
 */
static struct ip_vs_dest *
ip_vs_rr_schedule(struct ip_vs_service *svc, const struct sk_buff *skb)
{
	struct list_head *p, *q;
	struct ip_vs_dest *dest;

	IP_VS_DBG(6, "%s(): Scheduling...\n", __func__);

	write_lock(&svc->sched_lock);
	p = (struct list_head *)svc->sched_data;
	p = p->next;
	q = p;
	do {
		/* skip list head */
		if (q == &svc->destinations) {
			q = q->next;
			continue;
		}

		dest = list_entry(q, struct ip_vs_dest, n_list);
		if (!(dest->flags & IP_VS_DEST_F_OVERLOAD) &&
		    atomic_read(&dest->weight) > 0)
			/* HIT */
			goto out;
		q = q->next;
	} while (q != p);
	write_unlock(&svc->sched_lock);
	ip_vs_scheduler_err(svc, "no destination available");
	return NULL;

  out:
	svc->sched_data = q;
	write_unlock(&svc->sched_lock);
    ...
	return dest;
}
#+end_src
可以看出一种可能：所有后端都overload了。
另一种可能:destinations链表被清空。
第一种情况，由于现场没有设置threshold，所以可以忽略。
第二种情况，应该是由ipvsmon程序通过检测后端服务，发现后端服务没有及时响应时，把后端从ipvs中删除了。

** LVS和nginx引发的问题
  有人反映aaa/sigon请求超时。
  我写的小程序请求几个相关的URL，有几分之几的失败率。
  而另外/cam/user/getxxxx这个URL也超时。
  我猜测可能其他的URL也存在问题，不单是反映的aaa/sigon请求慢。
  于是在请求的URL后加上"&myseq=1"
  其中myseq的序号不停增加。
  看aaaservice日志和access.log日志
  发现access.log日志中收到的myseq序号有丢失的情况，
  而从nginx到aaaservice，没有丢包。
  
  nginx共有三台：nginx-a、ngnix-b、nginx-c
  其中nginx-c中 我发的特殊请求这种格式"myseq=", 一个都没有收到。
  

  怀疑是lvs的问题，又担心是测试机器到nginx之间网络问题，不确定。
  后来通过ipvsadmin -ln
  看
        ActiveConn  InactiveConn
  131    75          1000 
  141    945         1234
  145    879         1334

  ngnix-c上负载严重不均衡。
  

  想排除网络。
  于是在生产环境的局域网中一台机器中
  请求这URL: http://132.103.18.129:8080/cam/user/getxxxx?xxxx
  发现还是经常无影响。

  后来我想进一步确实问题，跟nginx转发无法，也就是与aaa无关。
  请求这个URL: http://132.103.18.129:8080/none
  故意让ngnix直接返回404，不启动转发功能，不与upstream server交互。
  发现还是经常连接不上。
  
  后来我想直接连接ngnix-a的ip
  http://132.103.18.141:8080/none
  很快响应。
  其他两台ngnix也同样测试了，没有问题。

  现在问题与LVS关系比较大了。

  但是没有直接证据，证明问题。
  Tony提议，抓包，
  在一台机器D上运行lwp-request -m GET http://132.103.18.129:8080/none
  
  在lvs上运行tcpdump -i eth0 tcp and host hostD-ip 
  抓数据。发现出问题时，确实有不断想机器D的syn请求重传。

  在ngnix-c上tcpdump -i tunl0 src hostD-ip
           tcpdump -i eth1 dst hostD-ip
  发现有收到lvs的syn请求，但是没有回应。

  后来重启了ngnix-c，问题消失。

  tony说可能是rp_filter的问题。

* rp_filter
** conf
The rp_filter can reject incoming packets if their source address doesn’t match the network interface that they’re arriving on, which helps to prevent IP spoofing. Turning this on, however, has its consequences: If your host has several IP addresses on different interfaces, or if your single interface has multiple IP addresses on it, you’ll find that your kernel may end up rejecting valid traffic. It’s also important to note that even if you do not enable the rp_filter, protection against broadcast spoofing is always on. Also, the protection it provides is only against spoofed internal addresses; external addresses can still be spoofed.. By default, it is disabled. To enable it, run the following:


rp_filter - INTEGER

 0 - No source validation.
 1 - Strict mode as defined in RFC3704 Strict Reverse Path
 Each incoming packet is tested against the FIB and if the interface
 is not the best reverse path the packet check will fail.
 By default failed packets are discarded.
 2 - Loose mode as defined in RFC3704 Loose Reverse Path
 Each incoming packet's source address is also tested against the FIB
 and if the source address is not reachable via any interface
 the packet check will fail.
Current recommended practice in RFC3704 is to enable strict mode
 to prevent IP spoofing from DDos attacks. If using asymmetric routing
 or other complicated routing, then loose mode is recommended.
The max value from conf/{all,interface}/rp_filter is used
 when doing source validation on the {interface}.
Default value is 0　based our os. Note that some distributions enable it
 in startup scripts.


 检查流入本机的 IP 地址是否合法，是否从对应接口的路由进来，是否是最佳路由。

设置方法：

系统配置文件
1. /etc/sysctl.conf


把 net.ipv4.conf.all.rp_filter和net.ipv4.conf.default.rp_filter设为0即可
net.ipv4.conf.default.rp_filter = 0
net.ipv4.conf.all.rp_filter = 0
系统启动后，会自动加载这个配置文件，内核会使用这个变量。

2. 命令行
显示一个内核变量 sysctl net.ipv4.conf.all.rp_filter
设置一个内核变量 sysctl -w net.ipv4.conf.all.rp_filter=0
设置完后，会更新内核（实时的内存）中的变量的值，但不会修改sysctl.conf的值

3. 使用/proc文件系统
查看 cat /proc/sys/net/ipv4/conf/all/rp_filter
设置 echo "0">/proc/sys/net/ipv4/conf/all/rp_filter
** code
kernel 3.0.13
#+begin_src c
/* Given (packet source, input interface) and optional (dst, oif, tos):
 * - (main) check, that source is valid i.e. not broadcast or our local
 *   address.
 * - figure out what "logical" interface this packet arrived
 *   and calculate "specific destination" address.
 * - check, that packet arrived from expected physical interface.
 * called with rcu_read_lock()
 */
int fib_validate_source(struct sk_buff *skb, __be32 src, __be32 dst, u8 tos,
			int oif, struct net_device *dev, __be32 *spec_dst,
			u32 *itag)
{
	struct in_device *in_dev;
	struct flowi4 fl4;
	struct fib_result res;
	int no_addr, rpf, accept_local;
	bool dev_match;
	int ret;
	struct net *net;

	fl4.flowi4_oif = 0;
	fl4.flowi4_iif = oif;
	fl4.daddr = src;
	fl4.saddr = dst;
	fl4.flowi4_tos = tos;
	fl4.flowi4_scope = RT_SCOPE_UNIVERSE;

	no_addr = rpf = accept_local = 0;
	in_dev = __in_dev_get_rcu(dev);
	if (in_dev) {
		no_addr = in_dev->ifa_list == NULL;

		/* Ignore rp_filter for packets protected by IPsec. */
		rpf = secpath_exists(skb) ? 0 : IN_DEV_RPFILTER(in_dev);

		accept_local = IN_DEV_ACCEPT_LOCAL(in_dev);
		fl4.flowi4_mark = IN_DEV_SRC_VMARK(in_dev) ? skb->mark : 0;
	}

	if (in_dev == NULL)
		goto e_inval;

	net = dev_net(dev);
	if (fib_lookup(net, &fl4, &res))
		goto last_resort;
	if (res.type != RTN_UNICAST) {
		if (res.type != RTN_LOCAL || !accept_local)
			goto e_inval;
	}
	*spec_dst = FIB_RES_PREFSRC(net, res);
	fib_combine_itag(itag, &res);
	dev_match = false;

#ifdef CONFIG_IP_ROUTE_MULTIPATH
	for (ret = 0; ret < res.fi->fib_nhs; ret++) {
		struct fib_nh *nh = &res.fi->fib_nh[ret];

		if (nh->nh_dev == dev) {
			dev_match = true;
			break;
		}
	}
#else
	if (FIB_RES_DEV(res) == dev)
		dev_match = true;
#endif
	if (dev_match) {
		ret = FIB_RES_NH(res).nh_scope >= RT_SCOPE_HOST;
		return ret;
	}
	if (no_addr)
		goto last_resort;
    // 这里可以看到当dev_match为false，也就是fib表认为不应该dev接受消息时，返回错误
	if (rpf == 1)
		goto e_rpf;
	fl4.flowi4_oif = dev->ifindex;

	ret = 0;
	if (fib_lookup(net, &fl4, &res) == 0) {
		if (res.type == RTN_UNICAST) {
			*spec_dst = FIB_RES_PREFSRC(net, res);
			ret = FIB_RES_NH(res).nh_scope >= RT_SCOPE_HOST;
		}
	}
	return ret;

last_resort:
	if (rpf)
		goto e_rpf;
	*spec_dst = inet_select_addr(dev, 0, RT_SCOPE_UNIVERSE);
	*itag = 0;
	return 0;

e_inval:
	return -EINVAL;
e_rpf:
	return -EXDEV;
}
#+end_src
** 查看命令
   使用netstat -st | grep IPReversePathFilter
   可以看到由于rp_filter过滤掉的数据包的个数

   这个值可以通过/proc/net/netstat查看
是由ip_rcv_finish中设置的
#+begin_src c
static int ip_rcv_finish(struct sk_buff *skb)
{
    ...
	if (skb_dst(skb) == NULL) {
		int err = ip_route_input_noref(skb, iph->daddr, iph->saddr,
					       iph->tos, skb->dev);
		if (unlikely(err)) {
			if (err == -EHOSTUNREACH)
				IP_INC_STATS_BH(dev_net(skb->dev),
						IPSTATS_MIB_INADDRERRORS);
			else if (err == -ENETUNREACH)
				IP_INC_STATS_BH(dev_net(skb->dev),
						IPSTATS_MIB_INNOROUTES);
            // 这里由于rp_filter过滤不过，返回的错误码
			else if (err == -EXDEV)
				NET_INC_STATS_BH(dev_net(skb->dev),
						 LINUX_MIB_IPRPFILTER);
			goto drop;
		}
	}
}
#+end_src
* tunnel
** code

Linux实现一个称为tunl的网络设备（类似loopback设备），此设备具有其他网络设备共有的特征，对于使用此设备的上层应用来说，对这些网络设备不加区分，调用及处理方法当然也完全一样。

我们知道，每一个IP数据包均交由ip_rcv函数处理，在进行一些必要的判断后，ip_rcv对于发送给本机的数据包将交给上层处理程序。
对于IPIP包来说，其处理函数是ipip_rcv（就如TCP包的处理函数是tcp_rcv一样，IP层不加区分）。
也就是说，当一个目的地址为本机的封包到达后，ip_rcv函数进行一些基本检查并除去IP头，然后交由ipip_rcv解封。
ipip_rcv所做的工作就是去掉封包头，还原数据包，然后把还原后的数据包放入相应的接收队列（netif_rx()）。


   /net/ipv4/tunnel4.c
#+begin_src c
static struct net_protocol tunnel4_protocol = {
	.handler	=	tunnel4_rcv,
	.err_handler	=	tunnel4_err,
	.no_policy	=	1,
};

static int __init tunnel4_init(void)
{
	if (inet_add_protocol(&tunnel4_protocol, IPPROTO_IPIP)) {
		printk(KERN_ERR "tunnel4 init: can't add protocol\n");
		return -EAGAIN;
	}
	return 0;
}
int xfrm4_tunnel_register(struct xfrm_tunnel *handler)
{
	struct xfrm_tunnel **pprev;
	int ret = -EEXIST;
	int priority = handler->priority;

	mutex_lock(&tunnel4_mutex);

	for (pprev = &tunnel4_handlers; *pprev; pprev = &(*pprev)->next) {
		if ((*pprev)->priority > priority)
			break;
		if ((*pprev)->priority == priority)
			goto err;
	}

	handler->next = *pprev;
	*pprev = handler;

	ret = 0;

err:
	mutex_unlock(&tunnel4_mutex);

	return ret;
}
static int tunnel4_rcv(struct sk_buff *skb)
{
	struct xfrm_tunnel *handler;

	if (!pskb_may_pull(skb, sizeof(struct iphdr)))
		goto drop;

	for (handler = tunnel4_handlers; handler; handler = handler->next)
		if (!handler->handler(skb))
			return 0;

	icmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, 0);

drop:
	kfree_skb(skb);
	return 0;
}
#+end_src
   /net/ipv4/ipip.c
#+begin_src c
static struct xfrm_tunnel ipip_handler = {
	.handler	=	ipip_rcv,
	.err_handler	=	ipip_err,
	.priority	=	1,
};
static int __init ipip_init(void)
{
	int err;

	printk(banner);

	if (xfrm4_tunnel_register(&ipip_handler)) {
		printk(KERN_INFO "ipip init: can't register tunnel\n");
		return -EAGAIN;
	}

	ipip_fb_tunnel_dev = alloc_netdev(sizeof(struct ip_tunnel),
					   "tunl0",
					   ipip_tunnel_setup);
	if (!ipip_fb_tunnel_dev) {
		err = -ENOMEM;
		goto err1;
	}

	ipip_fb_tunnel_dev->init = ipip_fb_tunnel_init;

	if ((err = register_netdev(ipip_fb_tunnel_dev)))
		goto err2;
 out:
	return err;
 err2:
	free_netdev(ipip_fb_tunnel_dev);
 err1:
	xfrm4_tunnel_deregister(&ipip_handler);
	goto out;
}

static int ipip_rcv(struct sk_buff *skb)
{
	struct iphdr *iph;
	struct ip_tunnel *tunnel;

	iph = skb->nh.iph;

	read_lock(&ipip_lock);
	if ((tunnel = ipip_tunnel_lookup(iph->saddr, iph->daddr)) != NULL) {
		if (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb)) {
			read_unlock(&ipip_lock);
			kfree_skb(skb);
			return 0;
		}

		secpath_reset(skb);

		skb->mac.raw = skb->nh.raw;
		skb->nh.raw = skb->data;
		skb->protocol = htons(ETH_P_IP);
		skb->pkt_type = PACKET_HOST;

		tunnel->stat.rx_packets++;
		tunnel->stat.rx_bytes += skb->len;
		skb->dev = tunnel->dev;
		dst_release(skb->dst);
		skb->dst = NULL;
		nf_reset(skb);
		ipip_ecn_decapsulate(iph, skb);
		netif_rx(skb);
		read_unlock(&ipip_lock);
		return 0;
	}
	read_unlock(&ipip_lock);

	return -1;
}

#+end_src
* lvs简单调优

1，调整ipvs connection hash表的大小

IPVS connection hash table size，取值范围:[12,20]。该表用于记录每个进来的连接及路由去向的信息。连接的Hash表要容纳几百万个并发连接，任何一个报文到达都需要查找连接Hash表，Hash表是系统使用最频繁的部分。Hash表的查找复杂度为O(n/m)，其中n为Hash表中对象的个数，m为Hash表的桶个数。当对象在Hash表中均匀分布和Hash表的桶个数与对象个数一样多时，Hash表的查找复杂度可以接近O(1)。

连接跟踪表中，每行称为一个hash bucket（hash桶），桶的个数是一个固定的值CONFIG_IP_VS_TAB_BITS，默认为12（2的12次方，4096）。这个值可以调整，该值的大小应该在 8 到 20 之间，详细的调整方法见后面。每一行都是一个链表结构，包含N列（即N条连接记录），这个N是无限的，N的数量决定了决定了查找的速度。

LVS的调优建议将hash table的值设置为不低于并发连接数。例如，并发连接数为200，Persistent时间为200S，那么hash桶的个数应设置为尽可能接近200x200=40000，2的15次方为32768就可以了。当ip_vs_conn_tab_bits=20 时，哈希表的的大小（条目）为 pow(2,20)，即 1048576，对于64位系统，IPVS占用大概16M内存，可以通过demsg看到：IPVS: Connection hash table configured (size=1048576, memory=16384Kbytes)。对于现在的服务器来说，这样的内存占用不是问题。所以直接设置为20即可。

关于最大“连接数限制”：这里的hash桶的个数，并不是LVS最大连接数限制。LVS使用哈希链表解决“哈希冲突”，当连接数大于这个值时，必然会出现哈稀冲突，会（稍微）降低性能，但是并不对在功能上对LVS造成影响。


调整 ip_vs_conn_tab_bits的方法：

新的IPVS代码，允许调整 ip_vs_conn_bits 的值。而老的IPVS代码则需要通过重新编译来调整。

在发行版里，IPVS通常是以模块的形式编译的。

确认能否调整使用命令 modinfo -p ip_vs（查看 ip_vs 模块的参数），看有没有 conn_tab_bits 参数可用。假如可以用，那么说时可以调整，调整方法是加载时通过设置 conn_tab_bits参数：

在/etc/modprobe.d/目录下添加文件ip_vs.conf，内容为：

options ip_vs conn_tab_bits=20

查看

ipvsadm -l

如果显示IP Virtual Server version 1.2.1 (size=4096),则前面加的参数没有生效

modprobe -r ip_vs

modprobe ip_vs

重新查看

IP Virtual Server version 1.2.1 (size=1048576)

假如没有 conn_tab_bits 参数可用，则需要重新调整编译选项，重新编译。

Centos6.2，内核版本2.6.32-220.13.1.el6.x86_64，仍然不支持这个参数，只能自定义编译了。

另外，假如IPVS支持调整 ip_vs_conn_tab_bits，而又将IPVS集成进了内核，那么只能通过重启，向内核传递参数来调整了。在引导程序的 kernel 相关的配置行上，添加：ip_vs.conn_tab_bits=20 ，然后，重启。

或者重新编译内核。


2，linux系统参数优化

关闭网卡LRO和GRO

现在大多数网卡都具有LRO/GRO功能，即 网卡收包时将同一流的小包合并成大包 （tcpdump抓包可以看到>MTU 1500bytes的数据包）交给 内核协议栈；LVS内核模块在处理>MTU的数据包时，会丢弃；

因此，如果我们用LVS来传输大文件，很容易出现丢包，传输速度慢；

解决方法，关闭LRO/GRO功能，命令：
#+begin_example
ethtool -k eth0 查看LRO/GRO当前是否打开

ethtool -K eth0 lro off 关闭GRO

ethtool -K eth0 gro off 关闭GRO
#+end_example
禁用ARP，增大backlog并发数
#+begin_example
net.ipv4.conf.all.arp_ignore = 1

net.ipv4.conf.all.arp_announce = 2

net.core.netdev_max_backlog = 500000  （在每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目）
#+end_example

3，lvs自身配置

尽量避免sh算法

一些业务为了支持会话保持，选择SH调度算法，以实现 同一源ip的请求调度到同一台RS上；但 SH算法本省没有实现一致性hash，一旦一台RS down，，当前所有连接都会断掉；如果配置了inhibit_on_failure，那就更悲剧了，调度到该RS上的流量会一直损失；

实际线上使用时，如需 会话保持，建议配置 persistence_timeout参数，保证一段时间同一源ip的请求到同一RS上。


4，手动绑定linux系统网卡中断
lvs的并发过大，对网卡的利用很频繁，而对网卡的调优，也能增加lvs的效率。当前大多数系统网卡都是支持硬件多队列的，为了充分发挥多核的性能，需要手动将网卡中断（流量）分配到所有CPU核上去处理。默认情况下，网卡的所有的中断都是发送到一个默认的cpu上去处理的，而cpu中断需要等待时间，这样对于使用网卡频繁的服务，网卡性能就会成为瓶颈。
1，查看网卡中断：
# cat /proc/interrupts

2，绑定网卡中断到cpu

例如将中断52-59分别绑定到CPU0-7上：

[plain] view plaincopy

echo "1" > /proc/irq/52/smp_affinity  

echo "2" > /proc/irq/53/smp_affinity  

echo "4" > /proc/irq/54/smp_affinity  

echo "8" > /proc/irq/55/smp_affinity  

echo "10" > /proc/irq/56/smp_affinity  

echo "20" > /proc/irq/57/smp_affinity  

echo "40" > /proc/irq/58/smp_affinity  

echo "80" > /proc/irq/59/smp_affinity  

/proc/irq/${IRQ_NUM}/smp_affinity为中断号为IRQ_NUM的中断绑定的CPU核的情况。以十六进制表示，每一位代表一个CPU核。

        1（00000001）代表CPU0

        2（00000010）代表CPU1

        3（00000011）代表CPU0和CPU1

3，关闭系统自动中断平衡：
# service irqbalance stop

4，如果网卡硬件不支持多队列，那就采用google提供的软多队列RPS；
配置方法同硬中断绑定，例：
# echo 01 > /sys/class/net/eth0/queues/rx-0/rps_cpus
# echo  02 > /sys/class/net/eth0/queues/rx-1/rps_cpus
* lvs 一个负载不均衡的问题
  ipvsadm -Ln
  显示一个IP的活动链接数是64,另一个IP的活动链接数是0.
  
  通过在LVS的director 和 RealServer 之间抓包发现确实不向其中一个IP转发。

  但是直接手动在
    
* 一个问题
  突然lvs后的两个real server收不到消息了。
  后来发现发往lvs的消息，都发往另一个机器了A。
  通过查看所有机器arp表，发现对应lvs的IP的mac，都是A机器的MAC，而且机器A之前配置过real server 存在相同的IP。
   原因不明
为了规避该问题：运行如下perl脚本，主动发起arp广播
while(1) {
   system("arping -A -c 1 -I eth2 -s 10.18.207.102 10.18.207.0 >/dev/null");
   sleep(1);
}  
* tcpdump 抓ipip包
  在realserver上
  在接收LVS消息的物理网口上
#+begin_example
  tcpdump -nn -i eth0 'ip[9] = 4'
#+end_example
  就可以看到IPIP报文
  在tunl0口上执行
  tcddump -nn -i tunl0 tcp port 80
  就可以看到IPIP报文剥除外层IP头后的，处理过程。
* DH
The dh scheduler only really works if the kernel can see the
destination address, what you need is for traffic passing through the
load balancer to be transparently load balanced to its destination....

So rather than clients requesting the load balancers VIP (virtual IP)...
You need to change the routing so that the clients request
www.microsoft.com or www.google.com directly BUT these requests are
routed through the load balancer....
Then you need to tell the load balancer to transparently intercept
that traffic with something like:

iptables -t mangle -A PREROUTING -p tcp --dport 80 -j MARK --set-mark 1
iptables -t mangle -A PREROUTING -p tcp --dport 443 -j MARK --set-mark 1
ip rule add prio 100 fwmark 1 table 100
ip route add local 0/0 dev lo table 100 
* LVS 连接超时与连接持久
LVS的持续时间有2个

1.把同一个client ip发来请求到同一台Real Server的持久超时时间。(即设置persistent的情况)

2.一个链接创建后空闲时的超时时间，这个超时时间分为3种。

1）TCP的空闲超时时间。

2）LVS收到客户端TCP FIN的超时时间

3）UDP的超时时间


第一种超时时间用 ipvsadm -p 时间来设置，如

#ipvsadm -A -t 192.168.20.154:80 -s rr -p 3600

设置超时时间为3600秒


查看用ipvsadm -L -n
#+begin_example
#ipvsadm
IP Virtual Server version x.x.x (size=4096)
Prot LocalAddress:Port Scheduler Flags
-> RemoteAddress:Port        Forward   Weight   ActiveConn    InActConn
FWM  99 rr persistent 3600
#+end_example
第二总超时时间用ipvsadm --set tcp tcpfin udp设置，比如

#ipvsadm --set 120 20 100

表示tcp空闲等待时间为120 秒

客户端关闭链接等待时间为20秒

udp空闲等待为100秒


可以通过ipvsadm -Lcn来查看
#+begin_example
pro expire state source virtual destination
TCP 00:27 NONE 192.168.8.107:0 192.168.20.154:80 192.168.20.194:80
TCP 00:07 ESTABLISHED 192.168.8.107:53432 192.168.20.154:80 192.168.20.194:80 
#+end_example

persistent timeout是连接模板的超时时间
而连接的超时时间仍然后普通的连接超时时间一样
见持久化调度代码：
#+begin_src c
static struct ip_vs_conn *
ip_vs_sched_persist(struct ip_vs_service *svc,
		    struct sk_buff *skb,
		    __be16 src_port, __be16 dst_port, int *ignored)
{
 ...
	/* Check if a template already exists */
	ct = ip_vs_ct_in_get(&param);
	if (!ct || !ip_vs_check_template(ct)) {
		/*
		 * No template found or the dest of the connection
		 * template is not available.
		 * return *ignored=0 i.e. ICMP and NF_DROP
		 */
		dest = svc->scheduler->schedule(svc, skb);
		if (!dest) {
			IP_VS_DBG(1, "p-schedule: no dest found.\n");
			kfree(param.pe_data);
			*ignored = 0;
			return NULL;
		}

		if (dst_port == svc->port && svc->port != FTPPORT)
			dport = dest->port;

		/* Create a template
		 * This adds param.pe_data to the template,
		 * and thus param.pe_data will be destroyed
		 * when the template expires */
		ct = ip_vs_conn_new(&param, &dest->addr, dport,
				    IP_VS_CONN_F_TEMPLATE, dest, skb->mark);
		if (ct == NULL) {
			kfree(param.pe_data);
			*ignored = -1;
			return NULL;
		}
        //重要一步：将connection template的超时时间设置为persistence timeout
		ct->timeout = svc->timeout;
	} else {
		/* set destination with the found template */
		dest = ct->dest;
		kfree(param.pe_data);
	}

	dport = dst_port;
	if (dport == svc->port && dest->port)
		dport = dest->port;

	flags = (svc->flags & IP_VS_SVC_F_ONEPACKET
		 && iph.protocol == IPPROTO_UDP)?
		IP_VS_CONN_F_ONE_PACKET : 0;

	/*
	 *    Create a new connection according to the template
	 */
	ip_vs_conn_fill_param(svc->net, svc->af, iph.protocol, &iph.saddr,
			      src_port, &iph.daddr, dst_port, &param);

	cp = ip_vs_conn_new(&param, &dest->addr, dport, flags, dest, skb->mark);
	if (cp == NULL) {
		ip_vs_conn_put(ct);
		*ignored = -1;
		return NULL;
	}

	/*
	 *    Add its control
	 */
	ip_vs_control_add(cp, ct);
	ip_vs_conn_put(ct);

	ip_vs_conn_stats(cp, svc);
	return cp;
}
#+end_src


其他链接超时：
#+begin_example
ip_vs_in()->
ip_vs_set_state() //更新TCP的状态，同时根据不同的状态设置不同的超时时间
ip_vs_conn_put()  //根据超时时间，重新设定连接定时器
#+end_example

#+begin_src c
/*
 *      Put back the conn and restart its timer with its timeout
 */
void ip_vs_conn_put(struct ip_vs_conn *cp)
{
	unsigned long t = (cp->flags & IP_VS_CONN_F_ONE_PACKET) ?
		0 : cp->timeout;
	mod_timer(&cp->timer, jiffies+t);

	__ip_vs_conn_put(cp);
}
#+end_src